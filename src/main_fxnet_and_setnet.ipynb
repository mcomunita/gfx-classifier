{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import dataset.dataset as dataset\n",
    "import datasplit.datasplit as datasplit\n",
    "import model.models as models\n",
    "import trainer.trainer as trainer\n",
    "import utils.utils as utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.device_count()\n",
    "\n",
    "cuda0 = torch.device('cuda:0')\n",
    "cuda1 = torch.device('cuda:1')\n",
    "cuda2 = torch.device('cuda:2')\n",
    "cuda3 = torch.device('cuda:3')\n",
    "\n",
    "device = torch.device(cuda0 if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforms\n",
    "transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "# dataset\n",
    "root = '/Volumes/Macintosh HD/DATASETS/GUITAR-FX/Mono'\n",
    "excl_folders = ['NoFX_mono', 'NoFX_mono_preprocessed']\n",
    "spectra_folder= 'mel_22050_1024_512'\n",
    "proc_settings_csv = 'proc_settings.csv'\n",
    "max_num_settings=6\n",
    "\n",
    "dataset = dataset.FxDataset(root=root,\n",
    "                            excl_folders=excl_folders, \n",
    "                            spectra_folder=spectra_folder, \n",
    "                            processed_settings_csv=proc_settings_csv,\n",
    "                            max_num_settings=max_num_settings,\n",
    "                            transform=transform)\n",
    "dataset.init_dataset()\n",
    "# dataset.generate_mel()\n",
    "\n",
    "# split\n",
    "split = datasplit.DataSplit(dataset, shuffle=True)\n",
    "# loaders\n",
    "train_loader, val_loader, test_loader = split.get_split(batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "163488"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAIN FxNET and SetNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "fxnet = models.FxNet().to(device)\n",
    "# optimizer\n",
    "optimizer_fxnet = optim.Adam(fxnet.parameters(), lr=0.001)\n",
    "# loss function\n",
    "loss_func_fxnet = nn.CrossEntropyLoss()\n",
    "\n",
    "# model\n",
    "setnet = models.SettingsNet().to(device)\n",
    "# optimizer\n",
    "optimizer_setnet = optim.Adam(setnet.parameters(), lr=0.001)\n",
    "# loss function\n",
    "loss_func_setnet = nn.MSELoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "FxNet(\n  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n  (conv2): Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1))\n  (fc1): Linear(in_features=6264, out_features=120, bias=True)\n  (fc2): Linear(in_features=120, out_features=60, bias=True)\n  (out): Linear(in_features=60, out_features=14, bias=True)\n)\nSettingsNet(\n  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n  (conv2): Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1))\n  (fc1): Linear(in_features=6264, out_features=120, bias=True)\n  (fc2): Linear(in_features=120, out_features=60, bias=True)\n  (out): Linear(in_features=60, out_features=6, bias=True)\n)\n"
    }
   ],
   "source": [
    "print(fxnet)\n",
    "print(setnet)"
   ]
  },
  {
   "source": [
    "# TRAIN and TEST FxNet OVER MULTIPLE EPOCHS\n",
    "train_set_size = len(split.train_sampler)\n",
    "val_set_size = len(split.val_sampler)\n",
    "test_set_size = len(split.test_sampler)\n",
    "\n",
    "all_train_losses_fx, all_val_losses_fx, all_test_losses_fx = [],[],[]\n",
    "all_train_correct_fx, all_val_correct_fx, all_test_correct_fx = [],[],[]\n",
    "all_train_results_fx, all_val_results_fx, all_test_results_fx = [],[],[]\n",
    "\n",
    "all_train_losses_set, all_val_losses_set, all_test_losses_set = [],[],[]\n",
    "all_train_correct_set, all_val_correct_set, all_test_correct_set = [],[],[]\n",
    "all_train_results_set, all_val_results_set, all_test_results_set = [],[],[]\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for epoch in range(50):\n",
    "    train_loss, train_correct, train_results = trainer.train_fx_net(\n",
    "        model=fxnet,\n",
    "        optimizer=optimizer_fxnet, \n",
    "        train_loader=train_loader, \n",
    "        train_sampler=split.train_sampler, \n",
    "        epoch=epoch, \n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    val_loss, val_correct, val_results = trainer.val_fx_net(\n",
    "        model=fxnet, \n",
    "        val_loader=val_loader, \n",
    "        val_sampler=split.val_sampler, \n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    test_loss, test_correct, test_results = trainer.test_fx_net(\n",
    "        model=fxnet, \n",
    "        test_loader=test_loader, \n",
    "        test_sampler=split.test_sampler, \n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    all_train_losses_fx.append(train_loss)\n",
    "    all_val_losses_fx.append(val_loss)\n",
    "    all_test_losses_fx.append(test_loss)\n",
    "    \n",
    "    all_train_correct_fx.append(train_correct)\n",
    "    all_val_correct_fx.append(val_correct)\n",
    "    all_test_correct_fx.append(test_correct)\n",
    "    \n",
    "    all_train_results_fx.append(train_results)\n",
    "    all_val_results_fx.append(val_results)\n",
    "    all_test_results_fx.append(test_results)\n",
    "\n",
    "    train_loss, train_correct, train_results = trainvaltest.train_settings_net(\n",
    "        model=setnet,\n",
    "        optimizer=optimizer_setnet, \n",
    "        train_loader=train_loader, \n",
    "        train_sampler=split.train_sampler, \n",
    "        epoch=epoch,\n",
    "        loss_function=loss_func_setnet, \n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    val_loss, val_correct, val_results = trainvaltest.val_settings_net(\n",
    "        model=setnet, \n",
    "        val_loader=val_loader, \n",
    "        val_sampler=split.val_sampler,\n",
    "        loss_function=loss_func_setnet,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    test_loss, test_correct, test_results = trainvaltest.test_settings_net(\n",
    "        model=setnet, \n",
    "        test_loader=test_loader, \n",
    "        test_sampler=split.test_sampler,\n",
    "        loss_function=loss_func_setnet,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    all_train_losses_set.append(train_loss)\n",
    "    all_val_losses_set.append(val_loss)\n",
    "    all_test_losses_set.append(test_loss)\n",
    "    \n",
    "    all_train_correct_set.append(train_correct)\n",
    "    all_val_correct_set.append(val_correct)\n",
    "    all_test_correct_set.append(test_correct)\n",
    "    \n",
    "    all_train_results_set.append(train_results)\n",
    "    all_val_results_set.append(val_results)\n",
    "    all_test_results_set.append(test_results)\n",
    "\n",
    "stop = time.time()\n",
    "print(f\"Training time: {stop - start}s\")"
   ],
   "cell_type": "code",
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "execution_count": 8,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "711 (78%)]\tTotal Loss: 4.6158\tAvg Loss: 0.0001\nTrain Epoch: 48\t[93000/117711 (79%)]\tTotal Loss: 4.6641\tAvg Loss: 0.0001\nTrain Epoch: 48\t[94000/117711 (80%)]\tTotal Loss: 4.7153\tAvg Loss: 0.0001\nTrain Epoch: 48\t[95000/117711 (81%)]\tTotal Loss: 4.7613\tAvg Loss: 0.0001\nTrain Epoch: 48\t[96000/117711 (81%)]\tTotal Loss: 4.8099\tAvg Loss: 0.0001\nTrain Epoch: 48\t[97000/117711 (82%)]\tTotal Loss: 4.8621\tAvg Loss: 0.0001\nTrain Epoch: 48\t[98000/117711 (83%)]\tTotal Loss: 4.9163\tAvg Loss: 0.0001\nTrain Epoch: 48\t[99000/117711 (84%)]\tTotal Loss: 4.9741\tAvg Loss: 0.0001\nTrain Epoch: 48\t[100000/117711 (85%)]\tTotal Loss: 5.0280\tAvg Loss: 0.0001\nTrain Epoch: 48\t[101000/117711 (86%)]\tTotal Loss: 5.0757\tAvg Loss: 0.0001\nTrain Epoch: 48\t[102000/117711 (87%)]\tTotal Loss: 5.1473\tAvg Loss: 0.0001\nTrain Epoch: 48\t[103000/117711 (87%)]\tTotal Loss: 5.2000\tAvg Loss: 0.0001\nTrain Epoch: 48\t[104000/117711 (88%)]\tTotal Loss: 5.2608\tAvg Loss: 0.0001\nTrain Epoch: 48\t[105000/117711 (89%)]\tTotal Loss: 5.3027\tAvg Loss: 0.0001\nTrain Epoch: 48\t[106000/117711 (90%)]\tTotal Loss: 5.3538\tAvg Loss: 0.0001\nTrain Epoch: 48\t[107000/117711 (91%)]\tTotal Loss: 5.4060\tAvg Loss: 0.0001\nTrain Epoch: 48\t[108000/117711 (92%)]\tTotal Loss: 5.4533\tAvg Loss: 0.0001\nTrain Epoch: 48\t[109000/117711 (93%)]\tTotal Loss: 5.5017\tAvg Loss: 0.0001\nTrain Epoch: 48\t[110000/117711 (93%)]\tTotal Loss: 5.5530\tAvg Loss: 0.0001\nTrain Epoch: 48\t[111000/117711 (94%)]\tTotal Loss: 5.6134\tAvg Loss: 0.0001\nTrain Epoch: 48\t[112000/117711 (95%)]\tTotal Loss: 5.6759\tAvg Loss: 0.0001\nTrain Epoch: 48\t[113000/117711 (96%)]\tTotal Loss: 5.7216\tAvg Loss: 0.0001\nTrain Epoch: 48\t[114000/117711 (97%)]\tTotal Loss: 5.7664\tAvg Loss: 0.0001\nTrain Epoch: 48\t[115000/117711 (98%)]\tTotal Loss: 5.8109\tAvg Loss: 0.0001\nTrain Epoch: 48\t[116000/117711 (98%)]\tTotal Loss: 5.8592\tAvg Loss: 0.0001\nTrain Epoch: 48\t[117000/117711 (99%)]\tTotal Loss: 5.9053\tAvg Loss: 0.0001\n====> Epoch: 48\tTotal Loss: 5.9324\t Avg Loss: 0.0001\tCorrect: 64714/117711\tPercentage Correct: 54.98\n====> Val Loss: 1.2057\t Avg Loss: 0.0001\tCorrect: 6402/13079\tPercentage Correct: 48.95\n====> Test Loss: 3.0738\t Avg Loss: 0.0001\tCorrect: 15825/32698\tPercentage Correct: 48.40\nTrain Epoch: 49\t[1000/117711 (1%)]\tTotal Loss: 2.8036\tAvg Loss: 0.0028\nTrain Epoch: 49\t[2000/117711 (2%)]\tTotal Loss: 4.3598\tAvg Loss: 0.0022\nTrain Epoch: 49\t[3000/117711 (3%)]\tTotal Loss: 5.7987\tAvg Loss: 0.0019\nTrain Epoch: 49\t[4000/117711 (3%)]\tTotal Loss: 7.2709\tAvg Loss: 0.0018\nTrain Epoch: 49\t[5000/117711 (4%)]\tTotal Loss: 8.7992\tAvg Loss: 0.0018\nTrain Epoch: 49\t[6000/117711 (5%)]\tTotal Loss: 10.0357\tAvg Loss: 0.0017\nTrain Epoch: 49\t[7000/117711 (6%)]\tTotal Loss: 11.4992\tAvg Loss: 0.0016\nTrain Epoch: 49\t[8000/117711 (7%)]\tTotal Loss: 13.0487\tAvg Loss: 0.0016\nTrain Epoch: 49\t[9000/117711 (8%)]\tTotal Loss: 14.4848\tAvg Loss: 0.0016\nTrain Epoch: 49\t[10000/117711 (8%)]\tTotal Loss: 15.9609\tAvg Loss: 0.0016\nTrain Epoch: 49\t[11000/117711 (9%)]\tTotal Loss: 17.4003\tAvg Loss: 0.0016\nTrain Epoch: 49\t[12000/117711 (10%)]\tTotal Loss: 18.7644\tAvg Loss: 0.0016\nTrain Epoch: 49\t[13000/117711 (11%)]\tTotal Loss: 20.0614\tAvg Loss: 0.0015\nTrain Epoch: 49\t[14000/117711 (12%)]\tTotal Loss: 21.5423\tAvg Loss: 0.0015\nTrain Epoch: 49\t[15000/117711 (13%)]\tTotal Loss: 22.9907\tAvg Loss: 0.0015\nTrain Epoch: 49\t[16000/117711 (14%)]\tTotal Loss: 24.2543\tAvg Loss: 0.0015\nTrain Epoch: 49\t[17000/117711 (14%)]\tTotal Loss: 25.6187\tAvg Loss: 0.0015\nTrain Epoch: 49\t[18000/117711 (15%)]\tTotal Loss: 27.0030\tAvg Loss: 0.0015\nTrain Epoch: 49\t[19000/117711 (16%)]\tTotal Loss: 28.3737\tAvg Loss: 0.0015\nTrain Epoch: 49\t[20000/117711 (17%)]\tTotal Loss: 29.6546\tAvg Loss: 0.0015\nTrain Epoch: 49\t[21000/117711 (18%)]\tTotal Loss: 31.2507\tAvg Loss: 0.0015\nTrain Epoch: 49\t[22000/117711 (19%)]\tTotal Loss: 32.4665\tAvg Loss: 0.0015\nTrain Epoch: 49\t[23000/117711 (20%)]\tTotal Loss: 33.9854\tAvg Loss: 0.0015\nTrain Epoch: 49\t[24000/117711 (20%)]\tTotal Loss: 35.5600\tAvg Loss: 0.0015\nTrain Epoch: 49\t[25000/117711 (21%)]\tTotal Loss: 37.0685\tAvg Loss: 0.0015\nTrain Epoch: 49\t[26000/117711 (22%)]\tTotal Loss: 39.0057\tAvg Loss: 0.0015\nTrain Epoch: 49\t[27000/117711 (23%)]\tTotal Loss: 40.7854\tAvg Loss: 0.0015\nTrain Epoch: 49\t[28000/117711 (24%)]\tTotal Loss: 42.0798\tAvg Loss: 0.0015\nTrain Epoch: 49\t[29000/117711 (25%)]\tTotal Loss: 43.7594\tAvg Loss: 0.0015\nTrain Epoch: 49\t[30000/117711 (25%)]\tTotal Loss: 45.0322\tAvg Loss: 0.0015\nTrain Epoch: 49\t[31000/117711 (26%)]\tTotal Loss: 46.4999\tAvg Loss: 0.0015\nTrain Epoch: 49\t[32000/117711 (27%)]\tTotal Loss: 48.1238\tAvg Loss: 0.0015\nTrain Epoch: 49\t[33000/117711 (28%)]\tTotal Loss: 49.4895\tAvg Loss: 0.0015\nTrain Epoch: 49\t[34000/117711 (29%)]\tTotal Loss: 50.7903\tAvg Loss: 0.0015\nTrain Epoch: 49\t[35000/117711 (30%)]\tTotal Loss: 52.2240\tAvg Loss: 0.0015\nTrain Epoch: 49\t[36000/117711 (31%)]\tTotal Loss: 53.5739\tAvg Loss: 0.0015\nTrain Epoch: 49\t[37000/117711 (31%)]\tTotal Loss: 54.9223\tAvg Loss: 0.0015\nTrain Epoch: 49\t[38000/117711 (32%)]\tTotal Loss: 56.2685\tAvg Loss: 0.0015\nTrain Epoch: 49\t[39000/117711 (33%)]\tTotal Loss: 57.7638\tAvg Loss: 0.0015\nTrain Epoch: 49\t[40000/117711 (34%)]\tTotal Loss: 59.0452\tAvg Loss: 0.0015\nTrain Epoch: 49\t[41000/117711 (35%)]\tTotal Loss: 60.4912\tAvg Loss: 0.0015\nTrain Epoch: 49\t[42000/117711 (36%)]\tTotal Loss: 61.9234\tAvg Loss: 0.0015\nTrain Epoch: 49\t[43000/117711 (37%)]\tTotal Loss: 63.2065\tAvg Loss: 0.0015\nTrain Epoch: 49\t[44000/117711 (37%)]\tTotal Loss: 64.7402\tAvg Loss: 0.0015\nTrain Epoch: 49\t[45000/117711 (38%)]\tTotal Loss: 66.3092\tAvg Loss: 0.0015\nTrain Epoch: 49\t[46000/117711 (39%)]\tTotal Loss: 67.7865\tAvg Loss: 0.0015\nTrain Epoch: 49\t[47000/117711 (40%)]\tTotal Loss: 69.2867\tAvg Loss: 0.0015\nTrain Epoch: 49\t[48000/117711 (41%)]\tTotal Loss: 70.6787\tAvg Loss: 0.0015\nTrain Epoch: 49\t[49000/117711 (42%)]\tTotal Loss: 72.2229\tAvg Loss: 0.0015\nTrain Epoch: 49\t[50000/117711 (42%)]\tTotal Loss: 73.5092\tAvg Loss: 0.0015\nTrain Epoch: 49\t[51000/117711 (43%)]\tTotal Loss: 74.9086\tAvg Loss: 0.0015\nTrain Epoch: 49\t[52000/117711 (44%)]\tTotal Loss: 76.2217\tAvg Loss: 0.0015\nTrain Epoch: 49\t[53000/117711 (45%)]\tTotal Loss: 77.5655\tAvg Loss: 0.0015\nTrain Epoch: 49\t[54000/117711 (46%)]\tTotal Loss: 79.1027\tAvg Loss: 0.0015\nTrain Epoch: 49\t[55000/117711 (47%)]\tTotal Loss: 80.5990\tAvg Loss: 0.0015\nTrain Epoch: 49\t[56000/117711 (48%)]\tTotal Loss: 81.9773\tAvg Loss: 0.0015\nTrain Epoch: 49\t[57000/117711 (48%)]\tTotal Loss: 83.3864\tAvg Loss: 0.0015\nTrain Epoch: 49\t[58000/117711 (49%)]\tTotal Loss: 84.9536\tAvg Loss: 0.0015\nTrain Epoch: 49\t[59000/117711 (50%)]\tTotal Loss: 86.4699\tAvg Loss: 0.0015\nTrain Epoch: 49\t[60000/117711 (51%)]\tTotal Loss: 87.8099\tAvg Loss: 0.0015\nTrain Epoch: 49\t[61000/117711 (52%)]\tTotal Loss: 89.1019\tAvg Loss: 0.0015\nTrain Epoch: 49\t[62000/117711 (53%)]\tTotal Loss: 90.4846\tAvg Loss: 0.0015\nTrain Epoch: 49\t[63000/117711 (53%)]\tTotal Loss: 91.9115\tAvg Loss: 0.0015\nTrain Epoch: 49\t[64000/117711 (54%)]\tTotal Loss: 93.3559\tAvg Loss: 0.0015\nTrain Epoch: 49\t[65000/117711 (55%)]\tTotal Loss: 94.7546\tAvg Loss: 0.0015\nTrain Epoch: 49\t[66000/117711 (56%)]\tTotal Loss: 96.2623\tAvg Loss: 0.0015\nTrain Epoch: 49\t[67000/117711 (57%)]\tTotal Loss: 97.5529\tAvg Loss: 0.0015\nTrain Epoch: 49\t[68000/117711 (58%)]\tTotal Loss: 98.7994\tAvg Loss: 0.0015\nTrain Epoch: 49\t[69000/117711 (59%)]\tTotal Loss: 100.0462\tAvg Loss: 0.0014\nTrain Epoch: 49\t[70000/117711 (59%)]\tTotal Loss: 101.4755\tAvg Loss: 0.0014\nTrain Epoch: 49\t[71000/117711 (60%)]\tTotal Loss: 102.7859\tAvg Loss: 0.0014\nTrain Epoch: 49\t[72000/117711 (61%)]\tTotal Loss: 104.0478\tAvg Loss: 0.0014\nTrain Epoch: 49\t[73000/117711 (62%)]\tTotal Loss: 105.5566\tAvg Loss: 0.0014\nTrain Epoch: 49\t[74000/117711 (63%)]\tTotal Loss: 106.6991\tAvg Loss: 0.0014\nTrain Epoch: 49\t[75000/117711 (64%)]\tTotal Loss: 107.9320\tAvg Loss: 0.0014\nTrain Epoch: 49\t[76000/117711 (65%)]\tTotal Loss: 109.2112\tAvg Loss: 0.0014\nTrain Epoch: 49\t[77000/117711 (65%)]\tTotal Loss: 110.5233\tAvg Loss: 0.0014\nTrain Epoch: 49\t[78000/117711 (66%)]\tTotal Loss: 111.9300\tAvg Loss: 0.0014\nTrain Epoch: 49\t[79000/117711 (67%)]\tTotal Loss: 113.2668\tAvg Loss: 0.0014\nTrain Epoch: 49\t[80000/117711 (68%)]\tTotal Loss: 114.9451\tAvg Loss: 0.0014\nTrain Epoch: 49\t[81000/117711 (69%)]\tTotal Loss: 116.3720\tAvg Loss: 0.0014\nTrain Epoch: 49\t[82000/117711 (70%)]\tTotal Loss: 117.6152\tAvg Loss: 0.0014\nTrain Epoch: 49\t[83000/117711 (70%)]\tTotal Loss: 119.2892\tAvg Loss: 0.0014\nTrain Epoch: 49\t[84000/117711 (71%)]\tTotal Loss: 120.6927\tAvg Loss: 0.0014\nTrain Epoch: 49\t[85000/117711 (72%)]\tTotal Loss: 122.1027\tAvg Loss: 0.0014\nTrain Epoch: 49\t[86000/117711 (73%)]\tTotal Loss: 123.4808\tAvg Loss: 0.0014\nTrain Epoch: 49\t[87000/117711 (74%)]\tTotal Loss: 125.4020\tAvg Loss: 0.0014\nTrain Epoch: 49\t[88000/117711 (75%)]\tTotal Loss: 126.7626\tAvg Loss: 0.0014\nTrain Epoch: 49\t[89000/117711 (76%)]\tTotal Loss: 128.1396\tAvg Loss: 0.0014\nTrain Epoch: 49\t[90000/117711 (76%)]\tTotal Loss: 129.9955\tAvg Loss: 0.0014\nTrain Epoch: 49\t[91000/117711 (77%)]\tTotal Loss: 131.3877\tAvg Loss: 0.0014\nTrain Epoch: 49\t[92000/117711 (78%)]\tTotal Loss: 132.9651\tAvg Loss: 0.0014\nTrain Epoch: 49\t[93000/117711 (79%)]\tTotal Loss: 134.5697\tAvg Loss: 0.0014\nTrain Epoch: 49\t[94000/117711 (80%)]\tTotal Loss: 136.0608\tAvg Loss: 0.0014\nTrain Epoch: 49\t[95000/117711 (81%)]\tTotal Loss: 137.5497\tAvg Loss: 0.0014\nTrain Epoch: 49\t[96000/117711 (81%)]\tTotal Loss: 139.1200\tAvg Loss: 0.0014\nTrain Epoch: 49\t[97000/117711 (82%)]\tTotal Loss: 140.9516\tAvg Loss: 0.0015\nTrain Epoch: 49\t[98000/117711 (83%)]\tTotal Loss: 142.8220\tAvg Loss: 0.0015\nTrain Epoch: 49\t[99000/117711 (84%)]\tTotal Loss: 144.3832\tAvg Loss: 0.0015\nTrain Epoch: 49\t[100000/117711 (85%)]\tTotal Loss: 145.8842\tAvg Loss: 0.0015\nTrain Epoch: 49\t[101000/117711 (86%)]\tTotal Loss: 147.3807\tAvg Loss: 0.0015\nTrain Epoch: 49\t[102000/117711 (87%)]\tTotal Loss: 148.7972\tAvg Loss: 0.0015\nTrain Epoch: 49\t[103000/117711 (87%)]\tTotal Loss: 150.0974\tAvg Loss: 0.0015\nTrain Epoch: 49\t[104000/117711 (88%)]\tTotal Loss: 151.4895\tAvg Loss: 0.0015\nTrain Epoch: 49\t[105000/117711 (89%)]\tTotal Loss: 152.7803\tAvg Loss: 0.0015\nTrain Epoch: 49\t[106000/117711 (90%)]\tTotal Loss: 154.1624\tAvg Loss: 0.0015\nTrain Epoch: 49\t[107000/117711 (91%)]\tTotal Loss: 155.9878\tAvg Loss: 0.0015\nTrain Epoch: 49\t[108000/117711 (92%)]\tTotal Loss: 157.2251\tAvg Loss: 0.0015\nTrain Epoch: 49\t[109000/117711 (93%)]\tTotal Loss: 158.7251\tAvg Loss: 0.0015\nTrain Epoch: 49\t[110000/117711 (93%)]\tTotal Loss: 160.2088\tAvg Loss: 0.0015\nTrain Epoch: 49\t[111000/117711 (94%)]\tTotal Loss: 161.5703\tAvg Loss: 0.0015\nTrain Epoch: 49\t[112000/117711 (95%)]\tTotal Loss: 162.7584\tAvg Loss: 0.0015\nTrain Epoch: 49\t[113000/117711 (96%)]\tTotal Loss: 164.3356\tAvg Loss: 0.0015\nTrain Epoch: 49\t[114000/117711 (97%)]\tTotal Loss: 165.5650\tAvg Loss: 0.0015\nTrain Epoch: 49\t[115000/117711 (98%)]\tTotal Loss: 167.1805\tAvg Loss: 0.0015\nTrain Epoch: 49\t[116000/117711 (98%)]\tTotal Loss: 168.4995\tAvg Loss: 0.0015\nTrain Epoch: 49\t[117000/117711 (99%)]\tTotal Loss: 169.9379\tAvg Loss: 0.0015\n====> Epoch: 49\tTotal Loss: 170.8558\t Avg Loss: 0.0015\tCorrect: 107052/117711\tPercentage Correct: 90.94\n====> Val Loss: 34.2187\t Avg Loss: 0.0026\tCorrect: 11638/13079\tPercentage Correct: 88.98\n====> Test Loss: 75.8076\t Avg Loss: 0.0023\tCorrect: 29128/32698\tPercentage Correct: 89.08\nTrain Epoch: 49\t[1000/117711 (1%)]\tTotal Loss: 0.0465\tAvg Loss: 0.0000\nTrain Epoch: 49\t[2000/117711 (2%)]\tTotal Loss: 0.0861\tAvg Loss: 0.0000\nTrain Epoch: 49\t[3000/117711 (3%)]\tTotal Loss: 0.1307\tAvg Loss: 0.0000\nTrain Epoch: 49\t[4000/117711 (3%)]\tTotal Loss: 0.1700\tAvg Loss: 0.0000\nTrain Epoch: 49\t[5000/117711 (4%)]\tTotal Loss: 0.2171\tAvg Loss: 0.0000\nTrain Epoch: 49\t[6000/117711 (5%)]\tTotal Loss: 0.2632\tAvg Loss: 0.0000\nTrain Epoch: 49\t[7000/117711 (6%)]\tTotal Loss: 0.3044\tAvg Loss: 0.0000\nTrain Epoch: 49\t[8000/117711 (7%)]\tTotal Loss: 0.3478\tAvg Loss: 0.0000\nTrain Epoch: 49\t[9000/117711 (8%)]\tTotal Loss: 0.3906\tAvg Loss: 0.0000\nTrain Epoch: 49\t[10000/117711 (8%)]\tTotal Loss: 0.4319\tAvg Loss: 0.0000\nTrain Epoch: 49\t[11000/117711 (9%)]\tTotal Loss: 0.4859\tAvg Loss: 0.0000\nTrain Epoch: 49\t[12000/117711 (10%)]\tTotal Loss: 0.5431\tAvg Loss: 0.0000\nTrain Epoch: 49\t[13000/117711 (11%)]\tTotal Loss: 0.5905\tAvg Loss: 0.0000\nTrain Epoch: 49\t[14000/117711 (12%)]\tTotal Loss: 0.6395\tAvg Loss: 0.0000\nTrain Epoch: 49\t[15000/117711 (13%)]\tTotal Loss: 0.6850\tAvg Loss: 0.0000\nTrain Epoch: 49\t[16000/117711 (14%)]\tTotal Loss: 0.7295\tAvg Loss: 0.0000\nTrain Epoch: 49\t[17000/117711 (14%)]\tTotal Loss: 0.7732\tAvg Loss: 0.0000\nTrain Epoch: 49\t[18000/117711 (15%)]\tTotal Loss: 0.8199\tAvg Loss: 0.0000\nTrain Epoch: 49\t[19000/117711 (16%)]\tTotal Loss: 0.8611\tAvg Loss: 0.0000\nTrain Epoch: 49\t[20000/117711 (17%)]\tTotal Loss: 0.9059\tAvg Loss: 0.0000\nTrain Epoch: 49\t[21000/117711 (18%)]\tTotal Loss: 0.9441\tAvg Loss: 0.0000\nTrain Epoch: 49\t[22000/117711 (19%)]\tTotal Loss: 0.9984\tAvg Loss: 0.0000\nTrain Epoch: 49\t[23000/117711 (20%)]\tTotal Loss: 1.0739\tAvg Loss: 0.0000\nTrain Epoch: 49\t[24000/117711 (20%)]\tTotal Loss: 1.1285\tAvg Loss: 0.0000\nTrain Epoch: 49\t[25000/117711 (21%)]\tTotal Loss: 1.1853\tAvg Loss: 0.0000\nTrain Epoch: 49\t[26000/117711 (22%)]\tTotal Loss: 1.2386\tAvg Loss: 0.0000\nTrain Epoch: 49\t[27000/117711 (23%)]\tTotal Loss: 1.2872\tAvg Loss: 0.0000\nTrain Epoch: 49\t[28000/117711 (24%)]\tTotal Loss: 1.3525\tAvg Loss: 0.0000\nTrain Epoch: 49\t[29000/117711 (25%)]\tTotal Loss: 1.4166\tAvg Loss: 0.0000\nTrain Epoch: 49\t[30000/117711 (25%)]\tTotal Loss: 1.4664\tAvg Loss: 0.0000\nTrain Epoch: 49\t[31000/117711 (26%)]\tTotal Loss: 1.5162\tAvg Loss: 0.0000\nTrain Epoch: 49\t[32000/117711 (27%)]\tTotal Loss: 1.5734\tAvg Loss: 0.0000\nTrain Epoch: 49\t[33000/117711 (28%)]\tTotal Loss: 1.6266\tAvg Loss: 0.0000\nTrain Epoch: 49\t[34000/117711 (29%)]\tTotal Loss: 1.6814\tAvg Loss: 0.0000\nTrain Epoch: 49\t[35000/117711 (30%)]\tTotal Loss: 1.7345\tAvg Loss: 0.0000\nTrain Epoch: 49\t[36000/117711 (31%)]\tTotal Loss: 1.7837\tAvg Loss: 0.0000\nTrain Epoch: 49\t[37000/117711 (31%)]\tTotal Loss: 1.8419\tAvg Loss: 0.0000\nTrain Epoch: 49\t[38000/117711 (32%)]\tTotal Loss: 1.8997\tAvg Loss: 0.0000\nTrain Epoch: 49\t[39000/117711 (33%)]\tTotal Loss: 1.9778\tAvg Loss: 0.0001\nTrain Epoch: 49\t[40000/117711 (34%)]\tTotal Loss: 2.0382\tAvg Loss: 0.0001\nTrain Epoch: 49\t[41000/117711 (35%)]\tTotal Loss: 2.1164\tAvg Loss: 0.0001\nTrain Epoch: 49\t[42000/117711 (36%)]\tTotal Loss: 2.1682\tAvg Loss: 0.0001\nTrain Epoch: 49\t[43000/117711 (37%)]\tTotal Loss: 2.2116\tAvg Loss: 0.0001\nTrain Epoch: 49\t[44000/117711 (37%)]\tTotal Loss: 2.2882\tAvg Loss: 0.0001\nTrain Epoch: 49\t[45000/117711 (38%)]\tTotal Loss: 2.3577\tAvg Loss: 0.0001\nTrain Epoch: 49\t[46000/117711 (39%)]\tTotal Loss: 2.4145\tAvg Loss: 0.0001\nTrain Epoch: 49\t[47000/117711 (40%)]\tTotal Loss: 2.4654\tAvg Loss: 0.0001\nTrain Epoch: 49\t[48000/117711 (41%)]\tTotal Loss: 2.5179\tAvg Loss: 0.0001\nTrain Epoch: 49\t[49000/117711 (42%)]\tTotal Loss: 2.5688\tAvg Loss: 0.0001\nTrain Epoch: 49\t[50000/117711 (42%)]\tTotal Loss: 2.6242\tAvg Loss: 0.0001\nTrain Epoch: 49\t[51000/117711 (43%)]\tTotal Loss: 2.6673\tAvg Loss: 0.0001\nTrain Epoch: 49\t[52000/117711 (44%)]\tTotal Loss: 2.7191\tAvg Loss: 0.0001\nTrain Epoch: 49\t[53000/117711 (45%)]\tTotal Loss: 2.7730\tAvg Loss: 0.0001\nTrain Epoch: 49\t[54000/117711 (46%)]\tTotal Loss: 2.8210\tAvg Loss: 0.0001\nTrain Epoch: 49\t[55000/117711 (47%)]\tTotal Loss: 2.8763\tAvg Loss: 0.0001\nTrain Epoch: 49\t[56000/117711 (48%)]\tTotal Loss: 2.9291\tAvg Loss: 0.0001\nTrain Epoch: 49\t[57000/117711 (48%)]\tTotal Loss: 2.9778\tAvg Loss: 0.0001\nTrain Epoch: 49\t[58000/117711 (49%)]\tTotal Loss: 3.0325\tAvg Loss: 0.0001\nTrain Epoch: 49\t[59000/117711 (50%)]\tTotal Loss: 3.0859\tAvg Loss: 0.0001\nTrain Epoch: 49\t[60000/117711 (51%)]\tTotal Loss: 3.1390\tAvg Loss: 0.0001\nTrain Epoch: 49\t[61000/117711 (52%)]\tTotal Loss: 3.2017\tAvg Loss: 0.0001\nTrain Epoch: 49\t[62000/117711 (53%)]\tTotal Loss: 3.2585\tAvg Loss: 0.0001\nTrain Epoch: 49\t[63000/117711 (53%)]\tTotal Loss: 3.3315\tAvg Loss: 0.0001\nTrain Epoch: 49\t[64000/117711 (54%)]\tTotal Loss: 3.3951\tAvg Loss: 0.0001\nTrain Epoch: 49\t[65000/117711 (55%)]\tTotal Loss: 3.4621\tAvg Loss: 0.0001\nTrain Epoch: 49\t[66000/117711 (56%)]\tTotal Loss: 3.5240\tAvg Loss: 0.0001\nTrain Epoch: 49\t[67000/117711 (57%)]\tTotal Loss: 3.5886\tAvg Loss: 0.0001\nTrain Epoch: 49\t[68000/117711 (58%)]\tTotal Loss: 3.6531\tAvg Loss: 0.0001\nTrain Epoch: 49\t[69000/117711 (59%)]\tTotal Loss: 3.7081\tAvg Loss: 0.0001\nTrain Epoch: 49\t[70000/117711 (59%)]\tTotal Loss: 3.7631\tAvg Loss: 0.0001\nTrain Epoch: 49\t[71000/117711 (60%)]\tTotal Loss: 3.8133\tAvg Loss: 0.0001\nTrain Epoch: 49\t[72000/117711 (61%)]\tTotal Loss: 3.8605\tAvg Loss: 0.0001\nTrain Epoch: 49\t[73000/117711 (62%)]\tTotal Loss: 3.9111\tAvg Loss: 0.0001\nTrain Epoch: 49\t[74000/117711 (63%)]\tTotal Loss: 3.9625\tAvg Loss: 0.0001\nTrain Epoch: 49\t[75000/117711 (64%)]\tTotal Loss: 4.0130\tAvg Loss: 0.0001\nTrain Epoch: 49\t[76000/117711 (65%)]\tTotal Loss: 4.0631\tAvg Loss: 0.0001\nTrain Epoch: 49\t[77000/117711 (65%)]\tTotal Loss: 4.1242\tAvg Loss: 0.0001\nTrain Epoch: 49\t[78000/117711 (66%)]\tTotal Loss: 4.1735\tAvg Loss: 0.0001\nTrain Epoch: 49\t[79000/117711 (67%)]\tTotal Loss: 4.2391\tAvg Loss: 0.0001\nTrain Epoch: 49\t[80000/117711 (68%)]\tTotal Loss: 4.2966\tAvg Loss: 0.0001\nTrain Epoch: 49\t[81000/117711 (69%)]\tTotal Loss: 4.3545\tAvg Loss: 0.0001\nTrain Epoch: 49\t[82000/117711 (70%)]\tTotal Loss: 4.4121\tAvg Loss: 0.0001\nTrain Epoch: 49\t[83000/117711 (70%)]\tTotal Loss: 4.4696\tAvg Loss: 0.0001\nTrain Epoch: 49\t[84000/117711 (71%)]\tTotal Loss: 4.5187\tAvg Loss: 0.0001\nTrain Epoch: 49\t[85000/117711 (72%)]\tTotal Loss: 4.5780\tAvg Loss: 0.0001\nTrain Epoch: 49\t[86000/117711 (73%)]\tTotal Loss: 4.6421\tAvg Loss: 0.0001\nTrain Epoch: 49\t[87000/117711 (74%)]\tTotal Loss: 4.7040\tAvg Loss: 0.0001\nTrain Epoch: 49\t[88000/117711 (75%)]\tTotal Loss: 4.7618\tAvg Loss: 0.0001\nTrain Epoch: 49\t[89000/117711 (76%)]\tTotal Loss: 4.8315\tAvg Loss: 0.0001\nTrain Epoch: 49\t[90000/117711 (76%)]\tTotal Loss: 4.8881\tAvg Loss: 0.0001\nTrain Epoch: 49\t[91000/117711 (77%)]\tTotal Loss: 4.9456\tAvg Loss: 0.0001\nTrain Epoch: 49\t[92000/117711 (78%)]\tTotal Loss: 5.0268\tAvg Loss: 0.0001\nTrain Epoch: 49\t[93000/117711 (79%)]\tTotal Loss: 5.0873\tAvg Loss: 0.0001\nTrain Epoch: 49\t[94000/117711 (80%)]\tTotal Loss: 5.1387\tAvg Loss: 0.0001\nTrain Epoch: 49\t[95000/117711 (81%)]\tTotal Loss: 5.1905\tAvg Loss: 0.0001\nTrain Epoch: 49\t[96000/117711 (81%)]\tTotal Loss: 5.2545\tAvg Loss: 0.0001\nTrain Epoch: 49\t[97000/117711 (82%)]\tTotal Loss: 5.3106\tAvg Loss: 0.0001\nTrain Epoch: 49\t[98000/117711 (83%)]\tTotal Loss: 5.3688\tAvg Loss: 0.0001\nTrain Epoch: 49\t[99000/117711 (84%)]\tTotal Loss: 5.4216\tAvg Loss: 0.0001\nTrain Epoch: 49\t[100000/117711 (85%)]\tTotal Loss: 5.5115\tAvg Loss: 0.0001\nTrain Epoch: 49\t[101000/117711 (86%)]\tTotal Loss: 5.5843\tAvg Loss: 0.0001\nTrain Epoch: 49\t[102000/117711 (87%)]\tTotal Loss: 5.7005\tAvg Loss: 0.0001\nTrain Epoch: 49\t[103000/117711 (87%)]\tTotal Loss: 5.8054\tAvg Loss: 0.0001\nTrain Epoch: 49\t[104000/117711 (88%)]\tTotal Loss: 5.9244\tAvg Loss: 0.0001\nTrain Epoch: 49\t[105000/117711 (89%)]\tTotal Loss: 6.0179\tAvg Loss: 0.0001\nTrain Epoch: 49\t[106000/117711 (90%)]\tTotal Loss: 6.1056\tAvg Loss: 0.0001\nTrain Epoch: 49\t[107000/117711 (91%)]\tTotal Loss: 6.2178\tAvg Loss: 0.0001\nTrain Epoch: 49\t[108000/117711 (92%)]\tTotal Loss: 6.3763\tAvg Loss: 0.0001\nTrain Epoch: 49\t[109000/117711 (93%)]\tTotal Loss: 6.5491\tAvg Loss: 0.0001\nTrain Epoch: 49\t[110000/117711 (93%)]\tTotal Loss: 6.6790\tAvg Loss: 0.0001\nTrain Epoch: 49\t[111000/117711 (94%)]\tTotal Loss: 6.8104\tAvg Loss: 0.0001\nTrain Epoch: 49\t[112000/117711 (95%)]\tTotal Loss: 6.9116\tAvg Loss: 0.0001\nTrain Epoch: 49\t[113000/117711 (96%)]\tTotal Loss: 7.0019\tAvg Loss: 0.0001\nTrain Epoch: 49\t[114000/117711 (97%)]\tTotal Loss: 7.0900\tAvg Loss: 0.0001\nTrain Epoch: 49\t[115000/117711 (98%)]\tTotal Loss: 7.1583\tAvg Loss: 0.0001\nTrain Epoch: 49\t[116000/117711 (98%)]\tTotal Loss: 7.2515\tAvg Loss: 0.0001\nTrain Epoch: 49\t[117000/117711 (99%)]\tTotal Loss: 7.3440\tAvg Loss: 0.0001\n====> Epoch: 49\tTotal Loss: 7.3933\t Avg Loss: 0.0001\tCorrect: 60847/117711\tPercentage Correct: 51.69\n====> Val Loss: 1.5842\t Avg Loss: 0.0001\tCorrect: 5592/13079\tPercentage Correct: 42.76\n====> Test Loss: 4.0419\t Avg Loss: 0.0001\tCorrect: 13825/32698\tPercentage Correct: 42.28\nTraining time: 91354.73111391068s\n"
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE\n",
    "models_folder = 'saved_models'\n",
    "model_name_fx = '20200924_fxnet'\n",
    "model_name_set = '20200924_setnet'\n",
    "results_folder = 'saved_results'\n",
    "results_subfolder = model_name_fx + '_and_setnet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE MODEL\n",
    "torch.save(fxnet, '%s/%s' % (models_folder, model_name_fx))\n",
    "torch.save(setnet, '%s/%s' % (models_folder, model_name_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE RESULTS\n",
    "all_train_losses_npy_fx = np.array(all_train_losses_fx)\n",
    "all_train_correct_npy_fx = np.array(all_train_correct_fx)\n",
    "all_train_results_npy_fx = np.array(all_train_results_fx)\n",
    "\n",
    "all_val_losses_npy_fx = np.array(all_val_losses_fx)\n",
    "all_val_correct_npy_fx = np.array(all_val_correct_fx)\n",
    "all_val_results_npy_fx = np.array(all_val_results_fx)\n",
    "\n",
    "all_test_losses_npy_fx = np.array(all_test_losses_fx)\n",
    "all_test_correct_npy_fx = np.array(all_test_correct_fx)\n",
    "all_test_results_npy_fx = np.array(all_test_results_fx)\n",
    "\n",
    "fx_labels_npy = np.array(list(dataset.fx_to_label.keys()))\n",
    "\n",
    "np.save(file=('%s/%s/%s' % (results_folder, results_subfolder, 'all_train_losses_fx')), arr=all_train_losses_npy_fx)\n",
    "np.save(file=('%s/%s/%s' % (results_folder, results_subfolder, 'all_train_correct_fx')), arr=all_train_correct_npy_fx)\n",
    "np.save(file=('%s/%s/%s' % (results_folder, results_subfolder, 'all_train_results_fx')), arr=all_train_results_npy_fx)\n",
    "\n",
    "np.save(file=('%s/%s/%s' % (results_folder, results_subfolder, 'all_val_losses_fx')), arr=all_val_losses_npy_fx)\n",
    "np.save(file=('%s/%s/%s' % (results_folder, results_subfolder, 'all_val_correct_fx')), arr=all_val_correct_npy_fx)\n",
    "np.save(file=('%s/%s/%s' % (results_folder, results_subfolder, 'all_val_results_fx')), arr=all_val_results_npy_fx)\n",
    "\n",
    "np.save(file=('%s/%s/%s' % (results_folder, results_subfolder, 'all_test_losses_fx')), arr=all_test_losses_npy_fx)\n",
    "np.save(file=('%s/%s/%s' % (results_folder, results_subfolder, 'all_test_correct_fx')), arr=all_test_correct_npy_fx)\n",
    "np.save(file=('%s/%s/%s' % (results_folder, results_subfolder, 'all_test_results_fx')), arr=all_test_results_npy_fx)\n",
    "\n",
    "np.save(file=('%s/%s/%s' % (results_folder, results_subfolder, 'fx_labels')), arr=fx_labels_npy)\n",
    "\n",
    "all_train_losses_npy_set = np.array(all_train_losses_set)\n",
    "all_train_correct_npy_set = np.array(all_train_correct_set)\n",
    "all_train_results_npy_set = np.array(all_train_results_set)\n",
    "\n",
    "all_val_losses_npy_set = np.array(all_val_losses_set)\n",
    "all_val_correct_npy_set = np.array(all_val_correct_set)\n",
    "all_val_results_npy_set = np.array(all_val_results_set)\n",
    "\n",
    "all_test_losses_npy_set = np.array(all_test_losses_set)\n",
    "all_test_correct_npy_set = np.array(all_test_correct_set)\n",
    "all_test_results_npy_set = np.array(all_test_results_set)\n",
    "\n",
    "np.save(file=('%s/%s/%s' % (results_folder, results_subfolder, 'all_train_losses_set')), arr=all_train_losses_npy_set)\n",
    "np.save(file=('%s/%s/%s' % (results_folder, results_subfolder, 'all_train_correct_set')), arr=all_train_correct_npy_set)\n",
    "np.save(file=('%s/%s/%s' % (results_folder, results_subfolder, 'all_train_results_set')), arr=all_train_results_npy_set)\n",
    "\n",
    "np.save(file=('%s/%s/%s' % (results_folder, results_subfolder, 'all_val_losses_set')), arr=all_val_losses_npy_set)\n",
    "np.save(file=('%s/%s/%s' % (results_folder, results_subfolder, 'all_val_correct_set')), arr=all_val_correct_npy_set)\n",
    "np.save(file=('%s/%s/%s' % (results_folder, results_subfolder, 'all_val_results_set')), arr=all_val_results_npy_set)\n",
    "\n",
    "np.save(file=('%s/%s/%s' % (results_folder, results_subfolder, 'all_test_losses_set')), arr=all_test_losses_npy_set)\n",
    "np.save(file=('%s/%s/%s' % (results_folder, results_subfolder, 'all_test_correct_set')), arr=all_test_correct_npy_set)\n",
    "np.save(file=('%s/%s/%s' % (results_folder, results_subfolder, 'all_test_results_set')), arr=all_test_results_npy_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "Python 3.8.5 64-bit ('venv')",
   "display_name": "Python 3.8.5 64-bit ('venv')",
   "metadata": {
    "interpreter": {
     "hash": "a7a805fa6ca9da60fb7b6b6395736daa5b09e73fa3d86545b482a0c8fc26d334"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}