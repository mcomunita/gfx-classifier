{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io, os, sys, types\n",
    "from IPython import get_ipython\n",
    "from nbformat import read\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "def find_notebook(fullname, path=None):\n",
    "    \"\"\"find a notebook, given its fully qualified name and an optional path\n",
    "    \n",
    "    This turns \"foo.bar\" into \"foo/bar.ipynb\"\n",
    "    and tries turning \"Foo_Bar\" into \"Foo Bar\" if Foo_Bar\n",
    "    does not exist.\n",
    "    \"\"\"\n",
    "    name = fullname.rsplit('.', 1)[-1]\n",
    "    if not path:\n",
    "        path = ['']\n",
    "    for d in path:\n",
    "        nb_path = os.path.join(d, name + \".ipynb\")\n",
    "        if os.path.isfile(nb_path):\n",
    "            return nb_path\n",
    "        # let import Notebook_Name find \"Notebook Name.ipynb\"\n",
    "        nb_path = nb_path.replace(\"_\", \" \")\n",
    "        if os.path.isfile(nb_path):\n",
    "            return nb_path\n",
    "\n",
    "\n",
    "class NotebookLoader(object):\n",
    "    \"\"\"Module Loader for Jupyter Notebooks\"\"\"\n",
    "    def __init__(self, path=None):\n",
    "        self.shell = InteractiveShell.instance()\n",
    "        self.path = path\n",
    "    \n",
    "    def load_module(self, fullname):\n",
    "        \"\"\"import a notebook as a module\"\"\"\n",
    "        path = find_notebook(fullname, self.path)\n",
    "        \n",
    "        print (\"importing Jupyter notebook from %s\" % path)\n",
    "                                       \n",
    "        # load the notebook object\n",
    "        with io.open(path, 'r', encoding='utf-8') as f:\n",
    "            nb = read(f, 4)\n",
    "        \n",
    "        \n",
    "        # create the module and add it to sys.modules\n",
    "        # if name in sys.modules:\n",
    "        #    return sys.modules[name]\n",
    "        mod = types.ModuleType(fullname)\n",
    "        mod.__file__ = path\n",
    "        mod.__loader__ = self\n",
    "        mod.__dict__['get_ipython'] = get_ipython\n",
    "        sys.modules[fullname] = mod\n",
    "        \n",
    "        # extra work to ensure that magics that would affect the user_ns\n",
    "        # actually affect the notebook module's ns\n",
    "        save_user_ns = self.shell.user_ns\n",
    "        self.shell.user_ns = mod.__dict__\n",
    "        \n",
    "        try:\n",
    "          for cell in nb.cells:\n",
    "            if cell.cell_type == 'code':\n",
    "                # transform the input to executable Python\n",
    "                code = self.shell.input_transformer_manager.transform_cell(cell.source)\n",
    "                # run the code in themodule\n",
    "                exec(code, mod.__dict__)\n",
    "        finally:\n",
    "            self.shell.user_ns = save_user_ns\n",
    "        return mod\n",
    "\n",
    "\n",
    "class NotebookFinder(object):\n",
    "    \"\"\"Module finder that locates Jupyter Notebooks\"\"\"\n",
    "    def __init__(self):\n",
    "        self.loaders = {}\n",
    "    \n",
    "    def find_module(self, fullname, path=None):\n",
    "        nb_path = find_notebook(fullname, path)\n",
    "        if not nb_path:\n",
    "            return\n",
    "        \n",
    "        key = path\n",
    "        if path:\n",
    "            # lists aren't hashable\n",
    "            key = os.path.sep.join(path)\n",
    "        \n",
    "        if key not in self.loaders:\n",
    "            self.loaders[key] = NotebookLoader(path)\n",
    "        return self.loaders[key]\n",
    "\n",
    "\n",
    "sys.meta_path.append(NotebookFinder())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import dataset\n",
    "import datasplit\n",
    "import models\n",
    "import trainvaltest\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.device_count()\n",
    "\n",
    "cuda0 = torch.device('cuda:0')\n",
    "cuda1 = torch.device('cuda:1')\n",
    "cuda2 = torch.device('cuda:2')\n",
    "cuda3 = torch.device('cuda:3')\n",
    "\n",
    "device = torch.device(cuda0 if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforms\n",
    "transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "# dataset\n",
    "root = '/Volumes/Macintosh HD/DATASETS/GUITAR-FX/Mono'\n",
    "excl_folders = ['NoFX_mono', 'NoFX_mono_preprocessed', '808', 'BD2', 'BMF', 'DPL',\n",
    "                 'DS1', 'FFC', 'MGS', 'OD1', 'RAT', 'RBM', 'SD1', 'TS9', 'VTB']\n",
    "spectra_folder= 'mel_22050_1024_512'\n",
    "proc_settings_csv = 'proc_settings.csv'\n",
    "max_num_settings=6\n",
    "\n",
    "dataset = dataset.FxDataset(root=root,\n",
    "                            excl_folders=excl_folders, \n",
    "                            spectra_folder=spectra_folder, \n",
    "                            processed_settings_csv=proc_settings_csv,\n",
    "                            max_num_settings=max_num_settings,\n",
    "                            transform=transform)\n",
    "dataset.init_dataset()\n",
    "# dataset.generate_mel()\n",
    "\n",
    "# split\n",
    "split = datasplit.DataSplit(dataset, shuffle=True)\n",
    "# loaders\n",
    "train_loader, val_loader, test_loader = split.get_split(batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "39936"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAIN FxNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "fxnet = models.FxNet().to(device)\n",
    "# optimizer\n",
    "optimizer_fxnet = optim.Adam(fxnet.parameters(), lr=0.001)\n",
    "# loss function\n",
    "loss_func_fxnet = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "FxNet(\n  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n  (conv2): Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1))\n  (fc1): Linear(in_features=6264, out_features=120, bias=True)\n  (fc2): Linear(in_features=120, out_features=60, bias=True)\n  (out): Linear(in_features=60, out_features=14, bias=True)\n)\n"
    }
   ],
   "source": [
    "print(fxnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.0012\nTrain Epoch: 46\t[58000/79228 (73%)]\tTotal Loss: 69.7318\tAvg Loss: 0.0012\nTrain Epoch: 46\t[59000/79228 (74%)]\tTotal Loss: 70.8550\tAvg Loss: 0.0012\nTrain Epoch: 46\t[60000/79228 (76%)]\tTotal Loss: 72.0533\tAvg Loss: 0.0012\nTrain Epoch: 46\t[61000/79228 (77%)]\tTotal Loss: 73.1320\tAvg Loss: 0.0012\nTrain Epoch: 46\t[62000/79228 (78%)]\tTotal Loss: 74.4023\tAvg Loss: 0.0012\nTrain Epoch: 46\t[63000/79228 (79%)]\tTotal Loss: 75.5204\tAvg Loss: 0.0012\nTrain Epoch: 46\t[64000/79228 (81%)]\tTotal Loss: 76.8072\tAvg Loss: 0.0012\nTrain Epoch: 46\t[65000/79228 (82%)]\tTotal Loss: 78.0436\tAvg Loss: 0.0012\nTrain Epoch: 46\t[66000/79228 (83%)]\tTotal Loss: 79.3618\tAvg Loss: 0.0012\nTrain Epoch: 46\t[67000/79228 (84%)]\tTotal Loss: 80.6868\tAvg Loss: 0.0012\nTrain Epoch: 46\t[68000/79228 (86%)]\tTotal Loss: 81.7708\tAvg Loss: 0.0012\nTrain Epoch: 46\t[69000/79228 (87%)]\tTotal Loss: 83.0891\tAvg Loss: 0.0012\nTrain Epoch: 46\t[70000/79228 (88%)]\tTotal Loss: 84.6155\tAvg Loss: 0.0012\nTrain Epoch: 46\t[71000/79228 (90%)]\tTotal Loss: 85.8542\tAvg Loss: 0.0012\nTrain Epoch: 46\t[72000/79228 (91%)]\tTotal Loss: 87.2365\tAvg Loss: 0.0012\nTrain Epoch: 46\t[73000/79228 (92%)]\tTotal Loss: 88.4996\tAvg Loss: 0.0012\nTrain Epoch: 46\t[74000/79228 (93%)]\tTotal Loss: 89.7694\tAvg Loss: 0.0012\nTrain Epoch: 46\t[75000/79228 (95%)]\tTotal Loss: 90.8331\tAvg Loss: 0.0012\nTrain Epoch: 46\t[76000/79228 (96%)]\tTotal Loss: 92.1191\tAvg Loss: 0.0012\nTrain Epoch: 46\t[77000/79228 (97%)]\tTotal Loss: 93.3558\tAvg Loss: 0.0012\nTrain Epoch: 46\t[78000/79228 (98%)]\tTotal Loss: 94.5854\tAvg Loss: 0.0012\nTrain Epoch: 46\t[79000/79228 (100%)]\tTotal Loss: 96.1309\tAvg Loss: 0.0012\n====> Epoch: 46\tTotal Loss: 96.4420\t Avg Loss: 0.0012\tCorrect: 72828/79228\tPercentage Correct: 91.92\n====> Val Loss: 12.6387\t Avg Loss: 0.0014\tCorrect: 8031/8804\tPercentage Correct: 91.22\n====> Test Loss: 31.5377\t Avg Loss: 0.0014\tCorrect: 20037/22008\tPercentage Correct: 91.04\nTrain Epoch: 47\t[1000/79228 (1%)]\tTotal Loss: 1.4069\tAvg Loss: 0.0014\nTrain Epoch: 47\t[2000/79228 (3%)]\tTotal Loss: 2.4605\tAvg Loss: 0.0012\nTrain Epoch: 47\t[3000/79228 (4%)]\tTotal Loss: 3.7735\tAvg Loss: 0.0013\nTrain Epoch: 47\t[4000/79228 (5%)]\tTotal Loss: 5.0103\tAvg Loss: 0.0013\nTrain Epoch: 47\t[5000/79228 (6%)]\tTotal Loss: 6.1225\tAvg Loss: 0.0012\nTrain Epoch: 47\t[6000/79228 (8%)]\tTotal Loss: 7.2258\tAvg Loss: 0.0012\nTrain Epoch: 47\t[7000/79228 (9%)]\tTotal Loss: 8.2519\tAvg Loss: 0.0012\nTrain Epoch: 47\t[8000/79228 (10%)]\tTotal Loss: 9.3691\tAvg Loss: 0.0012\nTrain Epoch: 47\t[9000/79228 (11%)]\tTotal Loss: 10.4228\tAvg Loss: 0.0012\nTrain Epoch: 47\t[10000/79228 (13%)]\tTotal Loss: 11.4207\tAvg Loss: 0.0011\nTrain Epoch: 47\t[11000/79228 (14%)]\tTotal Loss: 12.5057\tAvg Loss: 0.0011\nTrain Epoch: 47\t[12000/79228 (15%)]\tTotal Loss: 13.7883\tAvg Loss: 0.0011\nTrain Epoch: 47\t[13000/79228 (16%)]\tTotal Loss: 15.0702\tAvg Loss: 0.0012\nTrain Epoch: 47\t[14000/79228 (18%)]\tTotal Loss: 16.2937\tAvg Loss: 0.0012\nTrain Epoch: 47\t[15000/79228 (19%)]\tTotal Loss: 17.3931\tAvg Loss: 0.0012\nTrain Epoch: 47\t[16000/79228 (20%)]\tTotal Loss: 18.7415\tAvg Loss: 0.0012\nTrain Epoch: 47\t[17000/79228 (21%)]\tTotal Loss: 19.9784\tAvg Loss: 0.0012\nTrain Epoch: 47\t[18000/79228 (23%)]\tTotal Loss: 21.3360\tAvg Loss: 0.0012\nTrain Epoch: 47\t[19000/79228 (24%)]\tTotal Loss: 22.5526\tAvg Loss: 0.0012\nTrain Epoch: 47\t[20000/79228 (25%)]\tTotal Loss: 23.9650\tAvg Loss: 0.0012\nTrain Epoch: 47\t[21000/79228 (26%)]\tTotal Loss: 25.1912\tAvg Loss: 0.0012\nTrain Epoch: 47\t[22000/79228 (28%)]\tTotal Loss: 26.3830\tAvg Loss: 0.0012\nTrain Epoch: 47\t[23000/79228 (29%)]\tTotal Loss: 27.6634\tAvg Loss: 0.0012\nTrain Epoch: 47\t[24000/79228 (30%)]\tTotal Loss: 29.2078\tAvg Loss: 0.0012\nTrain Epoch: 47\t[25000/79228 (32%)]\tTotal Loss: 30.4231\tAvg Loss: 0.0012\nTrain Epoch: 47\t[26000/79228 (33%)]\tTotal Loss: 31.9121\tAvg Loss: 0.0012\nTrain Epoch: 47\t[27000/79228 (34%)]\tTotal Loss: 33.7624\tAvg Loss: 0.0013\nTrain Epoch: 47\t[28000/79228 (35%)]\tTotal Loss: 35.0180\tAvg Loss: 0.0013\nTrain Epoch: 47\t[29000/79228 (37%)]\tTotal Loss: 36.2551\tAvg Loss: 0.0013\nTrain Epoch: 47\t[30000/79228 (38%)]\tTotal Loss: 37.3554\tAvg Loss: 0.0012\nTrain Epoch: 47\t[31000/79228 (39%)]\tTotal Loss: 38.7024\tAvg Loss: 0.0012\nTrain Epoch: 47\t[32000/79228 (40%)]\tTotal Loss: 40.0758\tAvg Loss: 0.0013\nTrain Epoch: 47\t[33000/79228 (42%)]\tTotal Loss: 41.2401\tAvg Loss: 0.0012\nTrain Epoch: 47\t[34000/79228 (43%)]\tTotal Loss: 42.5459\tAvg Loss: 0.0013\nTrain Epoch: 47\t[35000/79228 (44%)]\tTotal Loss: 43.7101\tAvg Loss: 0.0012\nTrain Epoch: 47\t[36000/79228 (45%)]\tTotal Loss: 44.9596\tAvg Loss: 0.0012\nTrain Epoch: 47\t[37000/79228 (47%)]\tTotal Loss: 46.1407\tAvg Loss: 0.0012\nTrain Epoch: 47\t[38000/79228 (48%)]\tTotal Loss: 47.5145\tAvg Loss: 0.0013\nTrain Epoch: 47\t[39000/79228 (49%)]\tTotal Loss: 48.8374\tAvg Loss: 0.0013\nTrain Epoch: 47\t[40000/79228 (50%)]\tTotal Loss: 50.5984\tAvg Loss: 0.0013\nTrain Epoch: 47\t[41000/79228 (52%)]\tTotal Loss: 52.3438\tAvg Loss: 0.0013\nTrain Epoch: 47\t[42000/79228 (53%)]\tTotal Loss: 54.1738\tAvg Loss: 0.0013\nTrain Epoch: 47\t[43000/79228 (54%)]\tTotal Loss: 55.6729\tAvg Loss: 0.0013\nTrain Epoch: 47\t[44000/79228 (55%)]\tTotal Loss: 57.4328\tAvg Loss: 0.0013\nTrain Epoch: 47\t[45000/79228 (57%)]\tTotal Loss: 58.7792\tAvg Loss: 0.0013\nTrain Epoch: 47\t[46000/79228 (58%)]\tTotal Loss: 60.0860\tAvg Loss: 0.0013\nTrain Epoch: 47\t[47000/79228 (59%)]\tTotal Loss: 61.3403\tAvg Loss: 0.0013\nTrain Epoch: 47\t[48000/79228 (61%)]\tTotal Loss: 62.4934\tAvg Loss: 0.0013\nTrain Epoch: 47\t[49000/79228 (62%)]\tTotal Loss: 63.8965\tAvg Loss: 0.0013\nTrain Epoch: 47\t[50000/79228 (63%)]\tTotal Loss: 65.3197\tAvg Loss: 0.0013\nTrain Epoch: 47\t[51000/79228 (64%)]\tTotal Loss: 66.4718\tAvg Loss: 0.0013\nTrain Epoch: 47\t[52000/79228 (66%)]\tTotal Loss: 67.8505\tAvg Loss: 0.0013\nTrain Epoch: 47\t[53000/79228 (67%)]\tTotal Loss: 69.1032\tAvg Loss: 0.0013\nTrain Epoch: 47\t[54000/79228 (68%)]\tTotal Loss: 70.3436\tAvg Loss: 0.0013\nTrain Epoch: 47\t[55000/79228 (69%)]\tTotal Loss: 71.5175\tAvg Loss: 0.0013\nTrain Epoch: 47\t[56000/79228 (71%)]\tTotal Loss: 72.5310\tAvg Loss: 0.0013\nTrain Epoch: 47\t[57000/79228 (72%)]\tTotal Loss: 73.7797\tAvg Loss: 0.0013\nTrain Epoch: 47\t[58000/79228 (73%)]\tTotal Loss: 75.0234\tAvg Loss: 0.0013\nTrain Epoch: 47\t[59000/79228 (74%)]\tTotal Loss: 76.3836\tAvg Loss: 0.0013\nTrain Epoch: 47\t[60000/79228 (76%)]\tTotal Loss: 78.1483\tAvg Loss: 0.0013\nTrain Epoch: 47\t[61000/79228 (77%)]\tTotal Loss: 79.5455\tAvg Loss: 0.0013\nTrain Epoch: 47\t[62000/79228 (78%)]\tTotal Loss: 81.0002\tAvg Loss: 0.0013\nTrain Epoch: 47\t[63000/79228 (79%)]\tTotal Loss: 82.3589\tAvg Loss: 0.0013\nTrain Epoch: 47\t[64000/79228 (81%)]\tTotal Loss: 83.6861\tAvg Loss: 0.0013\nTrain Epoch: 47\t[65000/79228 (82%)]\tTotal Loss: 85.0200\tAvg Loss: 0.0013\nTrain Epoch: 47\t[66000/79228 (83%)]\tTotal Loss: 86.2740\tAvg Loss: 0.0013\nTrain Epoch: 47\t[67000/79228 (84%)]\tTotal Loss: 87.4302\tAvg Loss: 0.0013\nTrain Epoch: 47\t[68000/79228 (86%)]\tTotal Loss: 88.7738\tAvg Loss: 0.0013\nTrain Epoch: 47\t[69000/79228 (87%)]\tTotal Loss: 90.2328\tAvg Loss: 0.0013\nTrain Epoch: 47\t[70000/79228 (88%)]\tTotal Loss: 91.5876\tAvg Loss: 0.0013\nTrain Epoch: 47\t[71000/79228 (90%)]\tTotal Loss: 92.8755\tAvg Loss: 0.0013\nTrain Epoch: 47\t[72000/79228 (91%)]\tTotal Loss: 94.3463\tAvg Loss: 0.0013\nTrain Epoch: 47\t[73000/79228 (92%)]\tTotal Loss: 95.3486\tAvg Loss: 0.0013\nTrain Epoch: 47\t[74000/79228 (93%)]\tTotal Loss: 96.4161\tAvg Loss: 0.0013\nTrain Epoch: 47\t[75000/79228 (95%)]\tTotal Loss: 97.7244\tAvg Loss: 0.0013\nTrain Epoch: 47\t[76000/79228 (96%)]\tTotal Loss: 98.9961\tAvg Loss: 0.0013\nTrain Epoch: 47\t[77000/79228 (97%)]\tTotal Loss: 100.3561\tAvg Loss: 0.0013\nTrain Epoch: 47\t[78000/79228 (98%)]\tTotal Loss: 101.7046\tAvg Loss: 0.0013\nTrain Epoch: 47\t[79000/79228 (100%)]\tTotal Loss: 102.8554\tAvg Loss: 0.0013\n====> Epoch: 47\tTotal Loss: 103.0476\t Avg Loss: 0.0013\tCorrect: 72613/79228\tPercentage Correct: 91.65\n====> Val Loss: 13.3346\t Avg Loss: 0.0015\tCorrect: 8025/8804\tPercentage Correct: 91.15\n====> Test Loss: 33.1206\t Avg Loss: 0.0015\tCorrect: 20017/22008\tPercentage Correct: 90.95\nTrain Epoch: 48\t[1000/79228 (1%)]\tTotal Loss: 1.4254\tAvg Loss: 0.0014\nTrain Epoch: 48\t[2000/79228 (3%)]\tTotal Loss: 2.5156\tAvg Loss: 0.0013\nTrain Epoch: 48\t[3000/79228 (4%)]\tTotal Loss: 3.8635\tAvg Loss: 0.0013\nTrain Epoch: 48\t[4000/79228 (5%)]\tTotal Loss: 5.0585\tAvg Loss: 0.0013\nTrain Epoch: 48\t[5000/79228 (6%)]\tTotal Loss: 6.1630\tAvg Loss: 0.0012\nTrain Epoch: 48\t[6000/79228 (8%)]\tTotal Loss: 7.1977\tAvg Loss: 0.0012\nTrain Epoch: 48\t[7000/79228 (9%)]\tTotal Loss: 8.4044\tAvg Loss: 0.0012\nTrain Epoch: 48\t[8000/79228 (10%)]\tTotal Loss: 9.6822\tAvg Loss: 0.0012\nTrain Epoch: 48\t[9000/79228 (11%)]\tTotal Loss: 10.7308\tAvg Loss: 0.0012\nTrain Epoch: 48\t[10000/79228 (13%)]\tTotal Loss: 12.0626\tAvg Loss: 0.0012\nTrain Epoch: 48\t[11000/79228 (14%)]\tTotal Loss: 13.3361\tAvg Loss: 0.0012\nTrain Epoch: 48\t[12000/79228 (15%)]\tTotal Loss: 14.5622\tAvg Loss: 0.0012\nTrain Epoch: 48\t[13000/79228 (16%)]\tTotal Loss: 15.6451\tAvg Loss: 0.0012\nTrain Epoch: 48\t[14000/79228 (18%)]\tTotal Loss: 16.6658\tAvg Loss: 0.0012\nTrain Epoch: 48\t[15000/79228 (19%)]\tTotal Loss: 17.8565\tAvg Loss: 0.0012\nTrain Epoch: 48\t[16000/79228 (20%)]\tTotal Loss: 19.0056\tAvg Loss: 0.0012\nTrain Epoch: 48\t[17000/79228 (21%)]\tTotal Loss: 20.0744\tAvg Loss: 0.0012\nTrain Epoch: 48\t[18000/79228 (23%)]\tTotal Loss: 21.1322\tAvg Loss: 0.0012\nTrain Epoch: 48\t[19000/79228 (24%)]\tTotal Loss: 22.2034\tAvg Loss: 0.0012\nTrain Epoch: 48\t[20000/79228 (25%)]\tTotal Loss: 23.2360\tAvg Loss: 0.0012\nTrain Epoch: 48\t[21000/79228 (26%)]\tTotal Loss: 24.4020\tAvg Loss: 0.0012\nTrain Epoch: 48\t[22000/79228 (28%)]\tTotal Loss: 25.5626\tAvg Loss: 0.0012\nTrain Epoch: 48\t[23000/79228 (29%)]\tTotal Loss: 26.7470\tAvg Loss: 0.0012\nTrain Epoch: 48\t[24000/79228 (30%)]\tTotal Loss: 27.8594\tAvg Loss: 0.0012\nTrain Epoch: 48\t[25000/79228 (32%)]\tTotal Loss: 28.9580\tAvg Loss: 0.0012\nTrain Epoch: 48\t[26000/79228 (33%)]\tTotal Loss: 30.1789\tAvg Loss: 0.0012\nTrain Epoch: 48\t[27000/79228 (34%)]\tTotal Loss: 31.3777\tAvg Loss: 0.0012\nTrain Epoch: 48\t[28000/79228 (35%)]\tTotal Loss: 32.6636\tAvg Loss: 0.0012\nTrain Epoch: 48\t[29000/79228 (37%)]\tTotal Loss: 33.9790\tAvg Loss: 0.0012\nTrain Epoch: 48\t[30000/79228 (38%)]\tTotal Loss: 35.3344\tAvg Loss: 0.0012\nTrain Epoch: 48\t[31000/79228 (39%)]\tTotal Loss: 36.4922\tAvg Loss: 0.0012\nTrain Epoch: 48\t[32000/79228 (40%)]\tTotal Loss: 37.5365\tAvg Loss: 0.0012\nTrain Epoch: 48\t[33000/79228 (42%)]\tTotal Loss: 38.8078\tAvg Loss: 0.0012\nTrain Epoch: 48\t[34000/79228 (43%)]\tTotal Loss: 39.8917\tAvg Loss: 0.0012\nTrain Epoch: 48\t[35000/79228 (44%)]\tTotal Loss: 40.9182\tAvg Loss: 0.0012\nTrain Epoch: 48\t[36000/79228 (45%)]\tTotal Loss: 42.1595\tAvg Loss: 0.0012\nTrain Epoch: 48\t[37000/79228 (47%)]\tTotal Loss: 43.2189\tAvg Loss: 0.0012\nTrain Epoch: 48\t[38000/79228 (48%)]\tTotal Loss: 44.4363\tAvg Loss: 0.0012\nTrain Epoch: 48\t[39000/79228 (49%)]\tTotal Loss: 45.6054\tAvg Loss: 0.0012\nTrain Epoch: 48\t[40000/79228 (50%)]\tTotal Loss: 46.6840\tAvg Loss: 0.0012\nTrain Epoch: 48\t[41000/79228 (52%)]\tTotal Loss: 47.7907\tAvg Loss: 0.0012\nTrain Epoch: 48\t[42000/79228 (53%)]\tTotal Loss: 48.8826\tAvg Loss: 0.0012\nTrain Epoch: 48\t[43000/79228 (54%)]\tTotal Loss: 49.9990\tAvg Loss: 0.0012\nTrain Epoch: 48\t[44000/79228 (55%)]\tTotal Loss: 50.9689\tAvg Loss: 0.0012\nTrain Epoch: 48\t[45000/79228 (57%)]\tTotal Loss: 51.9961\tAvg Loss: 0.0012\nTrain Epoch: 48\t[46000/79228 (58%)]\tTotal Loss: 53.0597\tAvg Loss: 0.0012\nTrain Epoch: 48\t[47000/79228 (59%)]\tTotal Loss: 54.0452\tAvg Loss: 0.0011\nTrain Epoch: 48\t[48000/79228 (61%)]\tTotal Loss: 55.2185\tAvg Loss: 0.0012\nTrain Epoch: 48\t[49000/79228 (62%)]\tTotal Loss: 56.3285\tAvg Loss: 0.0011\nTrain Epoch: 48\t[50000/79228 (63%)]\tTotal Loss: 57.4594\tAvg Loss: 0.0011\nTrain Epoch: 48\t[51000/79228 (64%)]\tTotal Loss: 58.5994\tAvg Loss: 0.0011\nTrain Epoch: 48\t[52000/79228 (66%)]\tTotal Loss: 59.8543\tAvg Loss: 0.0012\nTrain Epoch: 48\t[53000/79228 (67%)]\tTotal Loss: 60.9830\tAvg Loss: 0.0012\nTrain Epoch: 48\t[54000/79228 (68%)]\tTotal Loss: 62.2917\tAvg Loss: 0.0012\nTrain Epoch: 48\t[55000/79228 (69%)]\tTotal Loss: 63.4684\tAvg Loss: 0.0012\nTrain Epoch: 48\t[56000/79228 (71%)]\tTotal Loss: 64.6773\tAvg Loss: 0.0012\nTrain Epoch: 48\t[57000/79228 (72%)]\tTotal Loss: 65.9870\tAvg Loss: 0.0012\nTrain Epoch: 48\t[58000/79228 (73%)]\tTotal Loss: 67.2612\tAvg Loss: 0.0012\nTrain Epoch: 48\t[59000/79228 (74%)]\tTotal Loss: 68.6040\tAvg Loss: 0.0012\nTrain Epoch: 48\t[60000/79228 (76%)]\tTotal Loss: 69.7614\tAvg Loss: 0.0012\nTrain Epoch: 48\t[61000/79228 (77%)]\tTotal Loss: 71.0732\tAvg Loss: 0.0012\nTrain Epoch: 48\t[62000/79228 (78%)]\tTotal Loss: 72.3863\tAvg Loss: 0.0012\nTrain Epoch: 48\t[63000/79228 (79%)]\tTotal Loss: 73.4128\tAvg Loss: 0.0012\nTrain Epoch: 48\t[64000/79228 (81%)]\tTotal Loss: 74.3510\tAvg Loss: 0.0012\nTrain Epoch: 48\t[65000/79228 (82%)]\tTotal Loss: 75.5875\tAvg Loss: 0.0012\nTrain Epoch: 48\t[66000/79228 (83%)]\tTotal Loss: 76.8180\tAvg Loss: 0.0012\nTrain Epoch: 48\t[67000/79228 (84%)]\tTotal Loss: 78.0239\tAvg Loss: 0.0012\nTrain Epoch: 48\t[68000/79228 (86%)]\tTotal Loss: 79.1901\tAvg Loss: 0.0012\nTrain Epoch: 48\t[69000/79228 (87%)]\tTotal Loss: 80.4400\tAvg Loss: 0.0012\nTrain Epoch: 48\t[70000/79228 (88%)]\tTotal Loss: 81.8006\tAvg Loss: 0.0012\nTrain Epoch: 48\t[71000/79228 (90%)]\tTotal Loss: 82.8428\tAvg Loss: 0.0012\nTrain Epoch: 48\t[72000/79228 (91%)]\tTotal Loss: 84.0912\tAvg Loss: 0.0012\nTrain Epoch: 48\t[73000/79228 (92%)]\tTotal Loss: 85.3294\tAvg Loss: 0.0012\nTrain Epoch: 48\t[74000/79228 (93%)]\tTotal Loss: 87.0295\tAvg Loss: 0.0012\nTrain Epoch: 48\t[75000/79228 (95%)]\tTotal Loss: 89.0500\tAvg Loss: 0.0012\nTrain Epoch: 48\t[76000/79228 (96%)]\tTotal Loss: 90.4177\tAvg Loss: 0.0012\nTrain Epoch: 48\t[77000/79228 (97%)]\tTotal Loss: 91.8351\tAvg Loss: 0.0012\nTrain Epoch: 48\t[78000/79228 (98%)]\tTotal Loss: 93.1806\tAvg Loss: 0.0012\nTrain Epoch: 48\t[79000/79228 (100%)]\tTotal Loss: 94.4554\tAvg Loss: 0.0012\n====> Epoch: 48\tTotal Loss: 94.7055\t Avg Loss: 0.0012\tCorrect: 72901/79228\tPercentage Correct: 92.01\n====> Val Loss: 15.6145\t Avg Loss: 0.0018\tCorrect: 7912/8804\tPercentage Correct: 89.87\n====> Test Loss: 40.7574\t Avg Loss: 0.0019\tCorrect: 19852/22008\tPercentage Correct: 90.20\nTrain Epoch: 49\t[1000/79228 (1%)]\tTotal Loss: 1.6190\tAvg Loss: 0.0016\nTrain Epoch: 49\t[2000/79228 (3%)]\tTotal Loss: 2.7321\tAvg Loss: 0.0014\nTrain Epoch: 49\t[3000/79228 (4%)]\tTotal Loss: 3.9892\tAvg Loss: 0.0013\nTrain Epoch: 49\t[4000/79228 (5%)]\tTotal Loss: 5.2576\tAvg Loss: 0.0013\nTrain Epoch: 49\t[5000/79228 (6%)]\tTotal Loss: 6.4176\tAvg Loss: 0.0013\nTrain Epoch: 49\t[6000/79228 (8%)]\tTotal Loss: 7.7011\tAvg Loss: 0.0013\nTrain Epoch: 49\t[7000/79228 (9%)]\tTotal Loss: 8.9276\tAvg Loss: 0.0013\nTrain Epoch: 49\t[8000/79228 (10%)]\tTotal Loss: 10.5782\tAvg Loss: 0.0013\nTrain Epoch: 49\t[9000/79228 (11%)]\tTotal Loss: 11.9118\tAvg Loss: 0.0013\nTrain Epoch: 49\t[10000/79228 (13%)]\tTotal Loss: 13.4326\tAvg Loss: 0.0013\nTrain Epoch: 49\t[11000/79228 (14%)]\tTotal Loss: 14.6095\tAvg Loss: 0.0013\nTrain Epoch: 49\t[12000/79228 (15%)]\tTotal Loss: 15.8383\tAvg Loss: 0.0013\nTrain Epoch: 49\t[13000/79228 (16%)]\tTotal Loss: 17.1023\tAvg Loss: 0.0013\nTrain Epoch: 49\t[14000/79228 (18%)]\tTotal Loss: 18.2984\tAvg Loss: 0.0013\nTrain Epoch: 49\t[15000/79228 (19%)]\tTotal Loss: 19.4709\tAvg Loss: 0.0013\nTrain Epoch: 49\t[16000/79228 (20%)]\tTotal Loss: 20.6159\tAvg Loss: 0.0013\nTrain Epoch: 49\t[17000/79228 (21%)]\tTotal Loss: 21.7941\tAvg Loss: 0.0013\nTrain Epoch: 49\t[18000/79228 (23%)]\tTotal Loss: 23.0193\tAvg Loss: 0.0013\nTrain Epoch: 49\t[19000/79228 (24%)]\tTotal Loss: 24.3061\tAvg Loss: 0.0013\nTrain Epoch: 49\t[20000/79228 (25%)]\tTotal Loss: 25.4497\tAvg Loss: 0.0013\nTrain Epoch: 49\t[21000/79228 (26%)]\tTotal Loss: 26.7092\tAvg Loss: 0.0013\nTrain Epoch: 49\t[22000/79228 (28%)]\tTotal Loss: 27.8597\tAvg Loss: 0.0013\nTrain Epoch: 49\t[23000/79228 (29%)]\tTotal Loss: 28.9396\tAvg Loss: 0.0013\nTrain Epoch: 49\t[24000/79228 (30%)]\tTotal Loss: 30.0177\tAvg Loss: 0.0013\nTrain Epoch: 49\t[25000/79228 (32%)]\tTotal Loss: 31.2244\tAvg Loss: 0.0012\nTrain Epoch: 49\t[26000/79228 (33%)]\tTotal Loss: 32.2624\tAvg Loss: 0.0012\nTrain Epoch: 49\t[27000/79228 (34%)]\tTotal Loss: 33.5517\tAvg Loss: 0.0012\nTrain Epoch: 49\t[28000/79228 (35%)]\tTotal Loss: 34.6813\tAvg Loss: 0.0012\nTrain Epoch: 49\t[29000/79228 (37%)]\tTotal Loss: 35.9082\tAvg Loss: 0.0012\nTrain Epoch: 49\t[30000/79228 (38%)]\tTotal Loss: 36.9744\tAvg Loss: 0.0012\nTrain Epoch: 49\t[31000/79228 (39%)]\tTotal Loss: 38.4325\tAvg Loss: 0.0012\nTrain Epoch: 49\t[32000/79228 (40%)]\tTotal Loss: 39.7453\tAvg Loss: 0.0012\nTrain Epoch: 49\t[33000/79228 (42%)]\tTotal Loss: 41.1507\tAvg Loss: 0.0012\nTrain Epoch: 49\t[34000/79228 (43%)]\tTotal Loss: 42.4349\tAvg Loss: 0.0012\nTrain Epoch: 49\t[35000/79228 (44%)]\tTotal Loss: 44.3162\tAvg Loss: 0.0013\nTrain Epoch: 49\t[36000/79228 (45%)]\tTotal Loss: 45.8014\tAvg Loss: 0.0013\nTrain Epoch: 49\t[37000/79228 (47%)]\tTotal Loss: 47.1008\tAvg Loss: 0.0013\nTrain Epoch: 49\t[38000/79228 (48%)]\tTotal Loss: 48.8716\tAvg Loss: 0.0013\nTrain Epoch: 49\t[39000/79228 (49%)]\tTotal Loss: 50.0652\tAvg Loss: 0.0013\nTrain Epoch: 49\t[40000/79228 (50%)]\tTotal Loss: 51.3280\tAvg Loss: 0.0013\nTrain Epoch: 49\t[41000/79228 (52%)]\tTotal Loss: 52.7508\tAvg Loss: 0.0013\nTrain Epoch: 49\t[42000/79228 (53%)]\tTotal Loss: 54.1025\tAvg Loss: 0.0013\nTrain Epoch: 49\t[43000/79228 (54%)]\tTotal Loss: 55.4525\tAvg Loss: 0.0013\nTrain Epoch: 49\t[44000/79228 (55%)]\tTotal Loss: 56.6925\tAvg Loss: 0.0013\nTrain Epoch: 49\t[45000/79228 (57%)]\tTotal Loss: 58.0387\tAvg Loss: 0.0013\nTrain Epoch: 49\t[46000/79228 (58%)]\tTotal Loss: 59.5157\tAvg Loss: 0.0013\nTrain Epoch: 49\t[47000/79228 (59%)]\tTotal Loss: 61.0130\tAvg Loss: 0.0013\nTrain Epoch: 49\t[48000/79228 (61%)]\tTotal Loss: 62.2869\tAvg Loss: 0.0013\nTrain Epoch: 49\t[49000/79228 (62%)]\tTotal Loss: 63.5785\tAvg Loss: 0.0013\nTrain Epoch: 49\t[50000/79228 (63%)]\tTotal Loss: 64.8199\tAvg Loss: 0.0013\nTrain Epoch: 49\t[51000/79228 (64%)]\tTotal Loss: 65.9913\tAvg Loss: 0.0013\nTrain Epoch: 49\t[52000/79228 (66%)]\tTotal Loss: 67.3297\tAvg Loss: 0.0013\nTrain Epoch: 49\t[53000/79228 (67%)]\tTotal Loss: 68.4813\tAvg Loss: 0.0013\nTrain Epoch: 49\t[54000/79228 (68%)]\tTotal Loss: 69.7297\tAvg Loss: 0.0013\nTrain Epoch: 49\t[55000/79228 (69%)]\tTotal Loss: 70.7342\tAvg Loss: 0.0013\nTrain Epoch: 49\t[56000/79228 (71%)]\tTotal Loss: 71.8158\tAvg Loss: 0.0013\nTrain Epoch: 49\t[57000/79228 (72%)]\tTotal Loss: 73.2714\tAvg Loss: 0.0013\nTrain Epoch: 49\t[58000/79228 (73%)]\tTotal Loss: 74.6167\tAvg Loss: 0.0013\nTrain Epoch: 49\t[59000/79228 (74%)]\tTotal Loss: 75.8110\tAvg Loss: 0.0013\nTrain Epoch: 49\t[60000/79228 (76%)]\tTotal Loss: 76.9212\tAvg Loss: 0.0013\nTrain Epoch: 49\t[61000/79228 (77%)]\tTotal Loss: 78.1642\tAvg Loss: 0.0013\nTrain Epoch: 49\t[62000/79228 (78%)]\tTotal Loss: 79.2198\tAvg Loss: 0.0013\nTrain Epoch: 49\t[63000/79228 (79%)]\tTotal Loss: 80.5085\tAvg Loss: 0.0013\nTrain Epoch: 49\t[64000/79228 (81%)]\tTotal Loss: 81.6267\tAvg Loss: 0.0013\nTrain Epoch: 49\t[65000/79228 (82%)]\tTotal Loss: 83.1223\tAvg Loss: 0.0013\nTrain Epoch: 49\t[66000/79228 (83%)]\tTotal Loss: 84.3703\tAvg Loss: 0.0013\nTrain Epoch: 49\t[67000/79228 (84%)]\tTotal Loss: 85.6470\tAvg Loss: 0.0013\nTrain Epoch: 49\t[68000/79228 (86%)]\tTotal Loss: 86.8327\tAvg Loss: 0.0013\nTrain Epoch: 49\t[69000/79228 (87%)]\tTotal Loss: 88.0239\tAvg Loss: 0.0013\nTrain Epoch: 49\t[70000/79228 (88%)]\tTotal Loss: 89.3584\tAvg Loss: 0.0013\nTrain Epoch: 49\t[71000/79228 (90%)]\tTotal Loss: 90.5582\tAvg Loss: 0.0013\nTrain Epoch: 49\t[72000/79228 (91%)]\tTotal Loss: 91.8775\tAvg Loss: 0.0013\nTrain Epoch: 49\t[73000/79228 (92%)]\tTotal Loss: 93.4152\tAvg Loss: 0.0013\nTrain Epoch: 49\t[74000/79228 (93%)]\tTotal Loss: 94.4766\tAvg Loss: 0.0013\nTrain Epoch: 49\t[75000/79228 (95%)]\tTotal Loss: 95.5836\tAvg Loss: 0.0013\nTrain Epoch: 49\t[76000/79228 (96%)]\tTotal Loss: 96.7272\tAvg Loss: 0.0013\nTrain Epoch: 49\t[77000/79228 (97%)]\tTotal Loss: 97.9545\tAvg Loss: 0.0013\nTrain Epoch: 49\t[78000/79228 (98%)]\tTotal Loss: 98.9589\tAvg Loss: 0.0013\nTrain Epoch: 49\t[79000/79228 (100%)]\tTotal Loss: 100.0949\tAvg Loss: 0.0013\n====> Epoch: 49\tTotal Loss: 100.3089\t Avg Loss: 0.0013\tCorrect: 72725/79228\tPercentage Correct: 91.79\n====> Val Loss: 13.4191\t Avg Loss: 0.0015\tCorrect: 8008/8804\tPercentage Correct: 90.96\n====> Test Loss: 31.6427\t Avg Loss: 0.0014\tCorrect: 20109/22008\tPercentage Correct: 91.37\nTraining time: 29196.531779050827s\n"
    }
   ],
   "source": [
    "# TRAIN and TEST FxNet OVER MULTIPLE EPOCHS\n",
    "train_set_size = len(split.train_sampler)\n",
    "val_set_size = len(split.val_sampler)\n",
    "test_set_size = len(split.test_sampler)\n",
    "\n",
    "all_train_losses, all_val_losses, all_test_losses = [],[],[]\n",
    "all_train_correct, all_val_correct, all_test_correct = [],[],[]\n",
    "all_train_results, all_val_results, all_test_results = [],[],[]\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for epoch in range(50):\n",
    "    train_loss, train_correct, train_results = trainvaltest.train_fx_net(\n",
    "        model=fxnet,\n",
    "        optimizer=optimizer_fxnet, \n",
    "        train_loader=train_loader, \n",
    "        train_sampler=split.train_sampler, \n",
    "        epoch=epoch, \n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    val_loss, val_correct, val_results = trainvaltest.val_fx_net(\n",
    "        model=fxnet, \n",
    "        val_loader=val_loader, \n",
    "        val_sampler=split.val_sampler, \n",
    "        epoch=epoch, \n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    test_loss, test_correct, test_results = trainvaltest.test_fx_net(\n",
    "        model=fxnet, \n",
    "        test_loader=test_loader, \n",
    "        test_sampler=split.test_sampler, \n",
    "        epoch=epoch, \n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    all_train_losses.append(train_loss)\n",
    "    all_val_losses.append(val_loss)\n",
    "    all_test_losses.append(test_loss)\n",
    "    \n",
    "    all_train_correct.append(train_correct)\n",
    "    all_val_correct.append(val_correct)\n",
    "    all_test_correct.append(test_correct)\n",
    "    \n",
    "    all_train_results.append(train_results)\n",
    "    all_val_results.append(val_results)\n",
    "    all_test_results.append(test_results)\n",
    "\n",
    "stop = time.time()\n",
    "print(f\"Training time: {stop - start}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE\n",
    "models_folder = 'saved_models'\n",
    "model_name = '20200828_fxnet'\n",
    "results_folder = 'saved_results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE MODEL\n",
    "torch.save(fxnet, '%s/%s' % (models_folder, model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE RESULTS\n",
    "all_train_losses_npy = np.array(all_train_losses)\n",
    "all_train_correct_npy = np.array(all_train_correct)\n",
    "all_train_results_npy = np.array(all_train_results)\n",
    "\n",
    "all_val_losses_npy = np.array(all_val_losses)\n",
    "all_val_correct_npy = np.array(all_val_correct)\n",
    "all_val_results_npy = np.array(all_val_results)\n",
    "\n",
    "all_test_losses_npy = np.array(all_test_losses)\n",
    "all_test_correct_npy = np.array(all_test_correct)\n",
    "all_test_results_npy = np.array(all_test_results)\n",
    "\n",
    "fx_labels_npy = np.array(list(dataset.fx_to_label.keys()))\n",
    "\n",
    "np.save(file=('%s/%s/%s' % (results_folder, model_name, 'all_train_losses')), arr=all_train_losses_npy)\n",
    "np.save(file=('%s/%s/%s' % (results_folder, model_name, 'all_train_correct')), arr=all_train_correct_npy)\n",
    "np.save(file=('%s/%s/%s' % (results_folder, model_name, 'all_train_results')), arr=all_train_results_npy)\n",
    "\n",
    "np.save(file=('%s/%s/%s' % (results_folder, model_name, 'all_val_losses')), arr=all_val_losses_npy)\n",
    "np.save(file=('%s/%s/%s' % (results_folder, model_name, 'all_val_correct')), arr=all_val_correct_npy)\n",
    "np.save(file=('%s/%s/%s' % (results_folder, model_name, 'all_val_results')), arr=all_val_results_npy)\n",
    "\n",
    "np.save(file=('%s/%s/%s' % (results_folder, model_name, 'all_test_losses')), arr=all_test_losses_npy)\n",
    "np.save(file=('%s/%s/%s' % (results_folder, model_name, 'all_test_correct')), arr=all_test_correct_npy)\n",
    "np.save(file=('%s/%s/%s' % (results_folder, model_name, 'all_test_results')), arr=all_test_results_npy)\n",
    "\n",
    "np.save(file=('%s/%s/%s' % (results_folder, model_name, 'fx_labels')), arr=fx_labels_npy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAIN SetNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "setnet = models.SettingsNet().to(device)\n",
    "# optimizer\n",
    "optimizer_setnet = optim.Adam(setnet.parameters(), lr=0.001)\n",
    "# loss function\n",
    "loss_func_setnet = nn.MSELoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "SettingsNet(\n  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n  (conv2): Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1))\n  (fc1): Linear(in_features=6264, out_features=120, bias=True)\n  (fc2): Linear(in_features=120, out_features=60, bias=True)\n  (out): Linear(in_features=60, out_features=6, bias=True)\n)\n"
    }
   ],
   "source": [
    "print(setnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "oss: 0.0561\tAvg Loss: 0.0000\nTrain Epoch: 41\t[9000/28753 (31%)]\tTotal Loss: 0.0621\tAvg Loss: 0.0000\nTrain Epoch: 41\t[10000/28753 (35%)]\tTotal Loss: 0.0722\tAvg Loss: 0.0000\nTrain Epoch: 41\t[11000/28753 (38%)]\tTotal Loss: 0.0832\tAvg Loss: 0.0000\nTrain Epoch: 41\t[12000/28753 (42%)]\tTotal Loss: 0.0911\tAvg Loss: 0.0000\nTrain Epoch: 41\t[13000/28753 (45%)]\tTotal Loss: 0.0985\tAvg Loss: 0.0000\nTrain Epoch: 41\t[14000/28753 (49%)]\tTotal Loss: 0.1051\tAvg Loss: 0.0000\nTrain Epoch: 41\t[15000/28753 (52%)]\tTotal Loss: 0.1117\tAvg Loss: 0.0000\nTrain Epoch: 41\t[16000/28753 (56%)]\tTotal Loss: 0.1203\tAvg Loss: 0.0000\nTrain Epoch: 41\t[17000/28753 (59%)]\tTotal Loss: 0.1311\tAvg Loss: 0.0000\nTrain Epoch: 41\t[18000/28753 (62%)]\tTotal Loss: 0.1394\tAvg Loss: 0.0000\nTrain Epoch: 41\t[19000/28753 (66%)]\tTotal Loss: 0.1465\tAvg Loss: 0.0000\nTrain Epoch: 41\t[20000/28753 (69%)]\tTotal Loss: 0.1548\tAvg Loss: 0.0000\nTrain Epoch: 41\t[21000/28753 (73%)]\tTotal Loss: 0.1627\tAvg Loss: 0.0000\nTrain Epoch: 41\t[22000/28753 (76%)]\tTotal Loss: 0.1733\tAvg Loss: 0.0000\nTrain Epoch: 41\t[23000/28753 (80%)]\tTotal Loss: 0.1811\tAvg Loss: 0.0000\nTrain Epoch: 41\t[24000/28753 (83%)]\tTotal Loss: 0.1886\tAvg Loss: 0.0000\nTrain Epoch: 41\t[25000/28753 (87%)]\tTotal Loss: 0.1956\tAvg Loss: 0.0000\nTrain Epoch: 41\t[26000/28753 (90%)]\tTotal Loss: 0.2021\tAvg Loss: 0.0000\nTrain Epoch: 41\t[27000/28753 (94%)]\tTotal Loss: 0.2091\tAvg Loss: 0.0000\nTrain Epoch: 41\t[28000/28753 (97%)]\tTotal Loss: 0.2158\tAvg Loss: 0.0000\n====> Epoch: 41\tTotal Loss: 0.2197\t Avg Loss: 0.0000\tCorrect: 25562/28753\tPercentage Correct: 88.90\n====> Val Loss: 0.0454\t Avg Loss: 0.0000\tCorrect: 2608/3195\tPercentage Correct: 81.63\n====> Test Loss: 0.1222\t Avg Loss: 0.0000\tCorrect: 6547/7988\tPercentage Correct: 81.96\nTrain Epoch: 42\t[1000/28753 (3%)]\tTotal Loss: 0.0076\tAvg Loss: 0.0000\nTrain Epoch: 42\t[2000/28753 (7%)]\tTotal Loss: 0.0130\tAvg Loss: 0.0000\nTrain Epoch: 42\t[3000/28753 (10%)]\tTotal Loss: 0.0184\tAvg Loss: 0.0000\nTrain Epoch: 42\t[4000/28753 (14%)]\tTotal Loss: 0.0241\tAvg Loss: 0.0000\nTrain Epoch: 42\t[5000/28753 (17%)]\tTotal Loss: 0.0295\tAvg Loss: 0.0000\nTrain Epoch: 42\t[6000/28753 (21%)]\tTotal Loss: 0.0381\tAvg Loss: 0.0000\nTrain Epoch: 42\t[7000/28753 (24%)]\tTotal Loss: 0.0461\tAvg Loss: 0.0000\nTrain Epoch: 42\t[8000/28753 (28%)]\tTotal Loss: 0.0523\tAvg Loss: 0.0000\nTrain Epoch: 42\t[9000/28753 (31%)]\tTotal Loss: 0.0583\tAvg Loss: 0.0000\nTrain Epoch: 42\t[10000/28753 (35%)]\tTotal Loss: 0.0643\tAvg Loss: 0.0000\nTrain Epoch: 42\t[11000/28753 (38%)]\tTotal Loss: 0.0709\tAvg Loss: 0.0000\nTrain Epoch: 42\t[12000/28753 (42%)]\tTotal Loss: 0.0785\tAvg Loss: 0.0000\nTrain Epoch: 42\t[13000/28753 (45%)]\tTotal Loss: 0.0935\tAvg Loss: 0.0000\nTrain Epoch: 42\t[14000/28753 (49%)]\tTotal Loss: 0.1030\tAvg Loss: 0.0000\nTrain Epoch: 42\t[15000/28753 (52%)]\tTotal Loss: 0.1108\tAvg Loss: 0.0000\nTrain Epoch: 42\t[16000/28753 (56%)]\tTotal Loss: 0.1194\tAvg Loss: 0.0000\nTrain Epoch: 42\t[17000/28753 (59%)]\tTotal Loss: 0.1287\tAvg Loss: 0.0000\nTrain Epoch: 42\t[18000/28753 (62%)]\tTotal Loss: 0.1385\tAvg Loss: 0.0000\nTrain Epoch: 42\t[19000/28753 (66%)]\tTotal Loss: 0.1466\tAvg Loss: 0.0000\nTrain Epoch: 42\t[20000/28753 (69%)]\tTotal Loss: 0.1541\tAvg Loss: 0.0000\nTrain Epoch: 42\t[21000/28753 (73%)]\tTotal Loss: 0.1611\tAvg Loss: 0.0000\nTrain Epoch: 42\t[22000/28753 (76%)]\tTotal Loss: 0.1700\tAvg Loss: 0.0000\nTrain Epoch: 42\t[23000/28753 (80%)]\tTotal Loss: 0.1783\tAvg Loss: 0.0000\nTrain Epoch: 42\t[24000/28753 (83%)]\tTotal Loss: 0.1849\tAvg Loss: 0.0000\nTrain Epoch: 42\t[25000/28753 (87%)]\tTotal Loss: 0.1916\tAvg Loss: 0.0000\nTrain Epoch: 42\t[26000/28753 (90%)]\tTotal Loss: 0.1977\tAvg Loss: 0.0000\nTrain Epoch: 42\t[27000/28753 (94%)]\tTotal Loss: 0.2039\tAvg Loss: 0.0000\nTrain Epoch: 42\t[28000/28753 (97%)]\tTotal Loss: 0.2105\tAvg Loss: 0.0000\n====> Epoch: 42\tTotal Loss: 0.2154\t Avg Loss: 0.0000\tCorrect: 25601/28753\tPercentage Correct: 89.04\n====> Val Loss: 0.0445\t Avg Loss: 0.0000\tCorrect: 2682/3195\tPercentage Correct: 83.94\n====> Test Loss: 0.1099\t Avg Loss: 0.0000\tCorrect: 6655/7988\tPercentage Correct: 83.31\nTrain Epoch: 43\t[1000/28753 (3%)]\tTotal Loss: 0.0064\tAvg Loss: 0.0000\nTrain Epoch: 43\t[2000/28753 (7%)]\tTotal Loss: 0.0114\tAvg Loss: 0.0000\nTrain Epoch: 43\t[3000/28753 (10%)]\tTotal Loss: 0.0168\tAvg Loss: 0.0000\nTrain Epoch: 43\t[4000/28753 (14%)]\tTotal Loss: 0.0220\tAvg Loss: 0.0000\nTrain Epoch: 43\t[5000/28753 (17%)]\tTotal Loss: 0.0269\tAvg Loss: 0.0000\nTrain Epoch: 43\t[6000/28753 (21%)]\tTotal Loss: 0.0329\tAvg Loss: 0.0000\nTrain Epoch: 43\t[7000/28753 (24%)]\tTotal Loss: 0.0408\tAvg Loss: 0.0000\nTrain Epoch: 43\t[8000/28753 (28%)]\tTotal Loss: 0.0470\tAvg Loss: 0.0000\nTrain Epoch: 43\t[9000/28753 (31%)]\tTotal Loss: 0.0545\tAvg Loss: 0.0000\nTrain Epoch: 43\t[10000/28753 (35%)]\tTotal Loss: 0.0610\tAvg Loss: 0.0000\nTrain Epoch: 43\t[11000/28753 (38%)]\tTotal Loss: 0.0679\tAvg Loss: 0.0000\nTrain Epoch: 43\t[12000/28753 (42%)]\tTotal Loss: 0.0771\tAvg Loss: 0.0000\nTrain Epoch: 43\t[13000/28753 (45%)]\tTotal Loss: 0.0860\tAvg Loss: 0.0000\nTrain Epoch: 43\t[14000/28753 (49%)]\tTotal Loss: 0.0938\tAvg Loss: 0.0000\nTrain Epoch: 43\t[15000/28753 (52%)]\tTotal Loss: 0.1020\tAvg Loss: 0.0000\nTrain Epoch: 43\t[16000/28753 (56%)]\tTotal Loss: 0.1116\tAvg Loss: 0.0000\nTrain Epoch: 43\t[17000/28753 (59%)]\tTotal Loss: 0.1210\tAvg Loss: 0.0000\nTrain Epoch: 43\t[18000/28753 (62%)]\tTotal Loss: 0.1335\tAvg Loss: 0.0000\nTrain Epoch: 43\t[19000/28753 (66%)]\tTotal Loss: 0.1420\tAvg Loss: 0.0000\nTrain Epoch: 43\t[20000/28753 (69%)]\tTotal Loss: 0.1486\tAvg Loss: 0.0000\nTrain Epoch: 43\t[21000/28753 (73%)]\tTotal Loss: 0.1547\tAvg Loss: 0.0000\nTrain Epoch: 43\t[22000/28753 (76%)]\tTotal Loss: 0.1620\tAvg Loss: 0.0000\nTrain Epoch: 43\t[23000/28753 (80%)]\tTotal Loss: 0.1683\tAvg Loss: 0.0000\nTrain Epoch: 43\t[24000/28753 (83%)]\tTotal Loss: 0.1745\tAvg Loss: 0.0000\nTrain Epoch: 43\t[25000/28753 (87%)]\tTotal Loss: 0.1805\tAvg Loss: 0.0000\nTrain Epoch: 43\t[26000/28753 (90%)]\tTotal Loss: 0.1867\tAvg Loss: 0.0000\nTrain Epoch: 43\t[27000/28753 (94%)]\tTotal Loss: 0.1924\tAvg Loss: 0.0000\nTrain Epoch: 43\t[28000/28753 (97%)]\tTotal Loss: 0.1999\tAvg Loss: 0.0000\n====> Epoch: 43\tTotal Loss: 0.2043\t Avg Loss: 0.0000\tCorrect: 25823/28753\tPercentage Correct: 89.81\n====> Val Loss: 0.0484\t Avg Loss: 0.0000\tCorrect: 2659/3195\tPercentage Correct: 83.22\n====> Test Loss: 0.1216\t Avg Loss: 0.0000\tCorrect: 6584/7988\tPercentage Correct: 82.42\nTrain Epoch: 44\t[1000/28753 (3%)]\tTotal Loss: 0.0077\tAvg Loss: 0.0000\nTrain Epoch: 44\t[2000/28753 (7%)]\tTotal Loss: 0.0135\tAvg Loss: 0.0000\nTrain Epoch: 44\t[3000/28753 (10%)]\tTotal Loss: 0.0204\tAvg Loss: 0.0000\nTrain Epoch: 44\t[4000/28753 (14%)]\tTotal Loss: 0.0266\tAvg Loss: 0.0000\nTrain Epoch: 44\t[5000/28753 (17%)]\tTotal Loss: 0.0315\tAvg Loss: 0.0000\nTrain Epoch: 44\t[6000/28753 (21%)]\tTotal Loss: 0.0370\tAvg Loss: 0.0000\nTrain Epoch: 44\t[7000/28753 (24%)]\tTotal Loss: 0.0434\tAvg Loss: 0.0000\nTrain Epoch: 44\t[8000/28753 (28%)]\tTotal Loss: 0.0516\tAvg Loss: 0.0000\nTrain Epoch: 44\t[9000/28753 (31%)]\tTotal Loss: 0.0627\tAvg Loss: 0.0000\nTrain Epoch: 44\t[10000/28753 (35%)]\tTotal Loss: 0.0736\tAvg Loss: 0.0000\nTrain Epoch: 44\t[11000/28753 (38%)]\tTotal Loss: 0.1023\tAvg Loss: 0.0000\nTrain Epoch: 44\t[12000/28753 (42%)]\tTotal Loss: 0.1363\tAvg Loss: 0.0000\nTrain Epoch: 44\t[13000/28753 (45%)]\tTotal Loss: 0.1608\tAvg Loss: 0.0000\nTrain Epoch: 44\t[14000/28753 (49%)]\tTotal Loss: 0.1995\tAvg Loss: 0.0000\nTrain Epoch: 44\t[15000/28753 (52%)]\tTotal Loss: 0.2700\tAvg Loss: 0.0000\nTrain Epoch: 44\t[16000/28753 (56%)]\tTotal Loss: 0.3325\tAvg Loss: 0.0000\nTrain Epoch: 44\t[17000/28753 (59%)]\tTotal Loss: 0.4145\tAvg Loss: 0.0000\nTrain Epoch: 44\t[18000/28753 (62%)]\tTotal Loss: 0.4678\tAvg Loss: 0.0000\nTrain Epoch: 44\t[19000/28753 (66%)]\tTotal Loss: 0.5240\tAvg Loss: 0.0000\nTrain Epoch: 44\t[20000/28753 (69%)]\tTotal Loss: 0.5796\tAvg Loss: 0.0000\nTrain Epoch: 44\t[21000/28753 (73%)]\tTotal Loss: 0.6734\tAvg Loss: 0.0000\nTrain Epoch: 44\t[22000/28753 (76%)]\tTotal Loss: 0.7373\tAvg Loss: 0.0000\nTrain Epoch: 44\t[23000/28753 (80%)]\tTotal Loss: 0.7851\tAvg Loss: 0.0000\nTrain Epoch: 44\t[24000/28753 (83%)]\tTotal Loss: 0.8220\tAvg Loss: 0.0000\nTrain Epoch: 44\t[25000/28753 (87%)]\tTotal Loss: 0.8580\tAvg Loss: 0.0000\nTrain Epoch: 44\t[26000/28753 (90%)]\tTotal Loss: 0.9016\tAvg Loss: 0.0000\nTrain Epoch: 44\t[27000/28753 (94%)]\tTotal Loss: 0.9495\tAvg Loss: 0.0000\nTrain Epoch: 44\t[28000/28753 (97%)]\tTotal Loss: 0.9884\tAvg Loss: 0.0000\n====> Epoch: 44\tTotal Loss: 1.0073\t Avg Loss: 0.0000\tCorrect: 21498/28753\tPercentage Correct: 74.77\n====> Val Loss: 0.0985\t Avg Loss: 0.0000\tCorrect: 2289/3195\tPercentage Correct: 71.64\n====> Test Loss: 0.2530\t Avg Loss: 0.0000\tCorrect: 5655/7988\tPercentage Correct: 70.79\nTrain Epoch: 45\t[1000/28753 (3%)]\tTotal Loss: 0.0243\tAvg Loss: 0.0000\nTrain Epoch: 45\t[2000/28753 (7%)]\tTotal Loss: 0.0436\tAvg Loss: 0.0000\nTrain Epoch: 45\t[3000/28753 (10%)]\tTotal Loss: 0.0605\tAvg Loss: 0.0000\nTrain Epoch: 45\t[4000/28753 (14%)]\tTotal Loss: 0.0805\tAvg Loss: 0.0000\nTrain Epoch: 45\t[5000/28753 (17%)]\tTotal Loss: 0.0994\tAvg Loss: 0.0000\nTrain Epoch: 45\t[6000/28753 (21%)]\tTotal Loss: 0.1162\tAvg Loss: 0.0000\nTrain Epoch: 45\t[7000/28753 (24%)]\tTotal Loss: 0.1327\tAvg Loss: 0.0000\nTrain Epoch: 45\t[8000/28753 (28%)]\tTotal Loss: 0.1562\tAvg Loss: 0.0000\nTrain Epoch: 45\t[9000/28753 (31%)]\tTotal Loss: 0.1754\tAvg Loss: 0.0000\nTrain Epoch: 45\t[10000/28753 (35%)]\tTotal Loss: 0.1937\tAvg Loss: 0.0000\nTrain Epoch: 45\t[11000/28753 (38%)]\tTotal Loss: 0.2126\tAvg Loss: 0.0000\nTrain Epoch: 45\t[12000/28753 (42%)]\tTotal Loss: 0.2335\tAvg Loss: 0.0000\nTrain Epoch: 45\t[13000/28753 (45%)]\tTotal Loss: 0.2504\tAvg Loss: 0.0000\nTrain Epoch: 45\t[14000/28753 (49%)]\tTotal Loss: 0.2651\tAvg Loss: 0.0000\nTrain Epoch: 45\t[15000/28753 (52%)]\tTotal Loss: 0.2835\tAvg Loss: 0.0000\nTrain Epoch: 45\t[16000/28753 (56%)]\tTotal Loss: 0.2985\tAvg Loss: 0.0000\nTrain Epoch: 45\t[17000/28753 (59%)]\tTotal Loss: 0.3123\tAvg Loss: 0.0000\nTrain Epoch: 45\t[18000/28753 (62%)]\tTotal Loss: 0.3266\tAvg Loss: 0.0000\nTrain Epoch: 45\t[19000/28753 (66%)]\tTotal Loss: 0.3442\tAvg Loss: 0.0000\nTrain Epoch: 45\t[20000/28753 (69%)]\tTotal Loss: 0.3600\tAvg Loss: 0.0000\nTrain Epoch: 45\t[21000/28753 (73%)]\tTotal Loss: 0.3719\tAvg Loss: 0.0000\nTrain Epoch: 45\t[22000/28753 (76%)]\tTotal Loss: 0.3876\tAvg Loss: 0.0000\nTrain Epoch: 45\t[23000/28753 (80%)]\tTotal Loss: 0.4025\tAvg Loss: 0.0000\nTrain Epoch: 45\t[24000/28753 (83%)]\tTotal Loss: 0.4128\tAvg Loss: 0.0000\nTrain Epoch: 45\t[25000/28753 (87%)]\tTotal Loss: 0.4226\tAvg Loss: 0.0000\nTrain Epoch: 45\t[26000/28753 (90%)]\tTotal Loss: 0.4381\tAvg Loss: 0.0000\nTrain Epoch: 45\t[27000/28753 (94%)]\tTotal Loss: 0.4600\tAvg Loss: 0.0000\nTrain Epoch: 45\t[28000/28753 (97%)]\tTotal Loss: 0.4748\tAvg Loss: 0.0000\n====> Epoch: 45\tTotal Loss: 0.4854\t Avg Loss: 0.0000\tCorrect: 23361/28753\tPercentage Correct: 81.25\n====> Val Loss: 0.0673\t Avg Loss: 0.0000\tCorrect: 2506/3195\tPercentage Correct: 78.44\n====> Test Loss: 0.1909\t Avg Loss: 0.0000\tCorrect: 6226/7988\tPercentage Correct: 77.94\nTrain Epoch: 46\t[1000/28753 (3%)]\tTotal Loss: 0.0177\tAvg Loss: 0.0000\nTrain Epoch: 46\t[2000/28753 (7%)]\tTotal Loss: 0.0305\tAvg Loss: 0.0000\nTrain Epoch: 46\t[3000/28753 (10%)]\tTotal Loss: 0.0446\tAvg Loss: 0.0000\nTrain Epoch: 46\t[4000/28753 (14%)]\tTotal Loss: 0.0629\tAvg Loss: 0.0000\nTrain Epoch: 46\t[5000/28753 (17%)]\tTotal Loss: 0.0764\tAvg Loss: 0.0000\nTrain Epoch: 46\t[6000/28753 (21%)]\tTotal Loss: 0.0971\tAvg Loss: 0.0000\nTrain Epoch: 46\t[7000/28753 (24%)]\tTotal Loss: 0.1159\tAvg Loss: 0.0000\nTrain Epoch: 46\t[8000/28753 (28%)]\tTotal Loss: 0.1308\tAvg Loss: 0.0000\nTrain Epoch: 46\t[9000/28753 (31%)]\tTotal Loss: 0.1509\tAvg Loss: 0.0000\nTrain Epoch: 46\t[10000/28753 (35%)]\tTotal Loss: 0.1627\tAvg Loss: 0.0000\nTrain Epoch: 46\t[11000/28753 (38%)]\tTotal Loss: 0.1749\tAvg Loss: 0.0000\nTrain Epoch: 46\t[12000/28753 (42%)]\tTotal Loss: 0.1856\tAvg Loss: 0.0000\nTrain Epoch: 46\t[13000/28753 (45%)]\tTotal Loss: 0.1960\tAvg Loss: 0.0000\nTrain Epoch: 46\t[14000/28753 (49%)]\tTotal Loss: 0.2044\tAvg Loss: 0.0000\nTrain Epoch: 46\t[15000/28753 (52%)]\tTotal Loss: 0.2150\tAvg Loss: 0.0000\nTrain Epoch: 46\t[16000/28753 (56%)]\tTotal Loss: 0.2234\tAvg Loss: 0.0000\nTrain Epoch: 46\t[17000/28753 (59%)]\tTotal Loss: 0.2344\tAvg Loss: 0.0000\nTrain Epoch: 46\t[18000/28753 (62%)]\tTotal Loss: 0.2418\tAvg Loss: 0.0000\nTrain Epoch: 46\t[19000/28753 (66%)]\tTotal Loss: 0.2496\tAvg Loss: 0.0000\nTrain Epoch: 46\t[20000/28753 (69%)]\tTotal Loss: 0.2584\tAvg Loss: 0.0000\nTrain Epoch: 46\t[21000/28753 (73%)]\tTotal Loss: 0.2661\tAvg Loss: 0.0000\nTrain Epoch: 46\t[22000/28753 (76%)]\tTotal Loss: 0.2758\tAvg Loss: 0.0000\nTrain Epoch: 46\t[23000/28753 (80%)]\tTotal Loss: 0.2826\tAvg Loss: 0.0000\nTrain Epoch: 46\t[24000/28753 (83%)]\tTotal Loss: 0.2900\tAvg Loss: 0.0000\nTrain Epoch: 46\t[25000/28753 (87%)]\tTotal Loss: 0.2972\tAvg Loss: 0.0000\nTrain Epoch: 46\t[26000/28753 (90%)]\tTotal Loss: 0.3047\tAvg Loss: 0.0000\nTrain Epoch: 46\t[27000/28753 (94%)]\tTotal Loss: 0.3121\tAvg Loss: 0.0000\nTrain Epoch: 46\t[28000/28753 (97%)]\tTotal Loss: 0.3198\tAvg Loss: 0.0000\n====> Epoch: 46\tTotal Loss: 0.3250\t Avg Loss: 0.0000\tCorrect: 24776/28753\tPercentage Correct: 86.17\n====> Val Loss: 0.0641\t Avg Loss: 0.0000\tCorrect: 2562/3195\tPercentage Correct: 80.19\n====> Test Loss: 0.1556\t Avg Loss: 0.0000\tCorrect: 6367/7988\tPercentage Correct: 79.71\nTrain Epoch: 47\t[1000/28753 (3%)]\tTotal Loss: 0.0112\tAvg Loss: 0.0000\nTrain Epoch: 47\t[2000/28753 (7%)]\tTotal Loss: 0.0203\tAvg Loss: 0.0000\nTrain Epoch: 47\t[3000/28753 (10%)]\tTotal Loss: 0.0285\tAvg Loss: 0.0000\nTrain Epoch: 47\t[4000/28753 (14%)]\tTotal Loss: 0.0352\tAvg Loss: 0.0000\nTrain Epoch: 47\t[5000/28753 (17%)]\tTotal Loss: 0.0439\tAvg Loss: 0.0000\nTrain Epoch: 47\t[6000/28753 (21%)]\tTotal Loss: 0.0502\tAvg Loss: 0.0000\nTrain Epoch: 47\t[7000/28753 (24%)]\tTotal Loss: 0.0578\tAvg Loss: 0.0000\nTrain Epoch: 47\t[8000/28753 (28%)]\tTotal Loss: 0.0656\tAvg Loss: 0.0000\nTrain Epoch: 47\t[9000/28753 (31%)]\tTotal Loss: 0.0712\tAvg Loss: 0.0000\nTrain Epoch: 47\t[10000/28753 (35%)]\tTotal Loss: 0.0780\tAvg Loss: 0.0000\nTrain Epoch: 47\t[11000/28753 (38%)]\tTotal Loss: 0.0856\tAvg Loss: 0.0000\nTrain Epoch: 47\t[12000/28753 (42%)]\tTotal Loss: 0.0940\tAvg Loss: 0.0000\nTrain Epoch: 47\t[13000/28753 (45%)]\tTotal Loss: 0.1021\tAvg Loss: 0.0000\nTrain Epoch: 47\t[14000/28753 (49%)]\tTotal Loss: 0.1091\tAvg Loss: 0.0000\nTrain Epoch: 47\t[15000/28753 (52%)]\tTotal Loss: 0.1167\tAvg Loss: 0.0000\nTrain Epoch: 47\t[16000/28753 (56%)]\tTotal Loss: 0.1240\tAvg Loss: 0.0000\nTrain Epoch: 47\t[17000/28753 (59%)]\tTotal Loss: 0.1307\tAvg Loss: 0.0000\nTrain Epoch: 47\t[18000/28753 (62%)]\tTotal Loss: 0.1372\tAvg Loss: 0.0000\nTrain Epoch: 47\t[19000/28753 (66%)]\tTotal Loss: 0.1438\tAvg Loss: 0.0000\nTrain Epoch: 47\t[20000/28753 (69%)]\tTotal Loss: 0.1514\tAvg Loss: 0.0000\nTrain Epoch: 47\t[21000/28753 (73%)]\tTotal Loss: 0.1584\tAvg Loss: 0.0000\nTrain Epoch: 47\t[22000/28753 (76%)]\tTotal Loss: 0.1644\tAvg Loss: 0.0000\nTrain Epoch: 47\t[23000/28753 (80%)]\tTotal Loss: 0.1757\tAvg Loss: 0.0000\nTrain Epoch: 47\t[24000/28753 (83%)]\tTotal Loss: 0.1842\tAvg Loss: 0.0000\nTrain Epoch: 47\t[25000/28753 (87%)]\tTotal Loss: 0.1934\tAvg Loss: 0.0000\nTrain Epoch: 47\t[26000/28753 (90%)]\tTotal Loss: 0.2006\tAvg Loss: 0.0000\nTrain Epoch: 47\t[27000/28753 (94%)]\tTotal Loss: 0.2126\tAvg Loss: 0.0000\nTrain Epoch: 47\t[28000/28753 (97%)]\tTotal Loss: 0.2256\tAvg Loss: 0.0000\n====> Epoch: 47\tTotal Loss: 0.2357\t Avg Loss: 0.0000\tCorrect: 25559/28753\tPercentage Correct: 88.89\n====> Val Loss: 0.0893\t Avg Loss: 0.0000\tCorrect: 2487/3195\tPercentage Correct: 77.84\n====> Test Loss: 0.2427\t Avg Loss: 0.0000\tCorrect: 6103/7988\tPercentage Correct: 76.40\nTrain Epoch: 48\t[1000/28753 (3%)]\tTotal Loss: 0.0172\tAvg Loss: 0.0000\nTrain Epoch: 48\t[2000/28753 (7%)]\tTotal Loss: 0.0295\tAvg Loss: 0.0000\nTrain Epoch: 48\t[3000/28753 (10%)]\tTotal Loss: 0.0425\tAvg Loss: 0.0000\nTrain Epoch: 48\t[4000/28753 (14%)]\tTotal Loss: 0.0533\tAvg Loss: 0.0000\nTrain Epoch: 48\t[5000/28753 (17%)]\tTotal Loss: 0.0638\tAvg Loss: 0.0000\nTrain Epoch: 48\t[6000/28753 (21%)]\tTotal Loss: 0.0759\tAvg Loss: 0.0000\nTrain Epoch: 48\t[7000/28753 (24%)]\tTotal Loss: 0.0845\tAvg Loss: 0.0000\nTrain Epoch: 48\t[8000/28753 (28%)]\tTotal Loss: 0.1010\tAvg Loss: 0.0000\nTrain Epoch: 48\t[9000/28753 (31%)]\tTotal Loss: 0.1108\tAvg Loss: 0.0000\nTrain Epoch: 48\t[10000/28753 (35%)]\tTotal Loss: 0.1223\tAvg Loss: 0.0000\nTrain Epoch: 48\t[11000/28753 (38%)]\tTotal Loss: 0.1313\tAvg Loss: 0.0000\nTrain Epoch: 48\t[12000/28753 (42%)]\tTotal Loss: 0.1436\tAvg Loss: 0.0000\nTrain Epoch: 48\t[13000/28753 (45%)]\tTotal Loss: 0.1547\tAvg Loss: 0.0000\nTrain Epoch: 48\t[14000/28753 (49%)]\tTotal Loss: 0.1649\tAvg Loss: 0.0000\nTrain Epoch: 48\t[15000/28753 (52%)]\tTotal Loss: 0.1746\tAvg Loss: 0.0000\nTrain Epoch: 48\t[16000/28753 (56%)]\tTotal Loss: 0.1897\tAvg Loss: 0.0000\nTrain Epoch: 48\t[17000/28753 (59%)]\tTotal Loss: 0.2078\tAvg Loss: 0.0000\nTrain Epoch: 48\t[18000/28753 (62%)]\tTotal Loss: 0.2289\tAvg Loss: 0.0000\nTrain Epoch: 48\t[19000/28753 (66%)]\tTotal Loss: 0.2466\tAvg Loss: 0.0000\nTrain Epoch: 48\t[20000/28753 (69%)]\tTotal Loss: 0.2583\tAvg Loss: 0.0000\nTrain Epoch: 48\t[21000/28753 (73%)]\tTotal Loss: 0.2676\tAvg Loss: 0.0000\nTrain Epoch: 48\t[22000/28753 (76%)]\tTotal Loss: 0.2763\tAvg Loss: 0.0000\nTrain Epoch: 48\t[23000/28753 (80%)]\tTotal Loss: 0.2881\tAvg Loss: 0.0000\nTrain Epoch: 48\t[24000/28753 (83%)]\tTotal Loss: 0.2965\tAvg Loss: 0.0000\nTrain Epoch: 48\t[25000/28753 (87%)]\tTotal Loss: 0.3058\tAvg Loss: 0.0000\nTrain Epoch: 48\t[26000/28753 (90%)]\tTotal Loss: 0.3145\tAvg Loss: 0.0000\nTrain Epoch: 48\t[27000/28753 (94%)]\tTotal Loss: 0.3234\tAvg Loss: 0.0000\nTrain Epoch: 48\t[28000/28753 (97%)]\tTotal Loss: 0.3312\tAvg Loss: 0.0000\n====> Epoch: 48\tTotal Loss: 0.3376\t Avg Loss: 0.0000\tCorrect: 24774/28753\tPercentage Correct: 86.16\n====> Val Loss: 0.0496\t Avg Loss: 0.0000\tCorrect: 2534/3195\tPercentage Correct: 79.31\n====> Test Loss: 0.1384\t Avg Loss: 0.0000\tCorrect: 6387/7988\tPercentage Correct: 79.96\nTrain Epoch: 49\t[1000/28753 (3%)]\tTotal Loss: 0.0091\tAvg Loss: 0.0000\nTrain Epoch: 49\t[2000/28753 (7%)]\tTotal Loss: 0.0160\tAvg Loss: 0.0000\nTrain Epoch: 49\t[3000/28753 (10%)]\tTotal Loss: 0.0238\tAvg Loss: 0.0000\nTrain Epoch: 49\t[4000/28753 (14%)]\tTotal Loss: 0.0299\tAvg Loss: 0.0000\nTrain Epoch: 49\t[5000/28753 (17%)]\tTotal Loss: 0.0363\tAvg Loss: 0.0000\nTrain Epoch: 49\t[6000/28753 (21%)]\tTotal Loss: 0.0427\tAvg Loss: 0.0000\nTrain Epoch: 49\t[7000/28753 (24%)]\tTotal Loss: 0.0496\tAvg Loss: 0.0000\nTrain Epoch: 49\t[8000/28753 (28%)]\tTotal Loss: 0.0557\tAvg Loss: 0.0000\nTrain Epoch: 49\t[9000/28753 (31%)]\tTotal Loss: 0.0614\tAvg Loss: 0.0000\nTrain Epoch: 49\t[10000/28753 (35%)]\tTotal Loss: 0.0670\tAvg Loss: 0.0000\nTrain Epoch: 49\t[11000/28753 (38%)]\tTotal Loss: 0.0773\tAvg Loss: 0.0000\nTrain Epoch: 49\t[12000/28753 (42%)]\tTotal Loss: 0.0868\tAvg Loss: 0.0000\nTrain Epoch: 49\t[13000/28753 (45%)]\tTotal Loss: 0.1054\tAvg Loss: 0.0000\nTrain Epoch: 49\t[14000/28753 (49%)]\tTotal Loss: 0.1227\tAvg Loss: 0.0000\nTrain Epoch: 49\t[15000/28753 (52%)]\tTotal Loss: 0.1455\tAvg Loss: 0.0000\nTrain Epoch: 49\t[16000/28753 (56%)]\tTotal Loss: 0.1685\tAvg Loss: 0.0000\nTrain Epoch: 49\t[17000/28753 (59%)]\tTotal Loss: 0.1990\tAvg Loss: 0.0000\nTrain Epoch: 49\t[18000/28753 (62%)]\tTotal Loss: 0.2237\tAvg Loss: 0.0000\nTrain Epoch: 49\t[19000/28753 (66%)]\tTotal Loss: 0.2557\tAvg Loss: 0.0000\nTrain Epoch: 49\t[20000/28753 (69%)]\tTotal Loss: 0.2792\tAvg Loss: 0.0000\nTrain Epoch: 49\t[21000/28753 (73%)]\tTotal Loss: 0.3029\tAvg Loss: 0.0000\nTrain Epoch: 49\t[22000/28753 (76%)]\tTotal Loss: 0.3309\tAvg Loss: 0.0000\nTrain Epoch: 49\t[23000/28753 (80%)]\tTotal Loss: 0.3623\tAvg Loss: 0.0000\nTrain Epoch: 49\t[24000/28753 (83%)]\tTotal Loss: 0.3861\tAvg Loss: 0.0000\nTrain Epoch: 49\t[25000/28753 (87%)]\tTotal Loss: 0.4212\tAvg Loss: 0.0000\nTrain Epoch: 49\t[26000/28753 (90%)]\tTotal Loss: 0.4462\tAvg Loss: 0.0000\nTrain Epoch: 49\t[27000/28753 (94%)]\tTotal Loss: 0.4738\tAvg Loss: 0.0000\nTrain Epoch: 49\t[28000/28753 (97%)]\tTotal Loss: 0.4934\tAvg Loss: 0.0000\n====> Epoch: 49\tTotal Loss: 0.5039\t Avg Loss: 0.0000\tCorrect: 24002/28753\tPercentage Correct: 83.48\n====> Val Loss: 0.0880\t Avg Loss: 0.0000\tCorrect: 2408/3195\tPercentage Correct: 75.37\n====> Test Loss: 0.2317\t Avg Loss: 0.0000\tCorrect: 6105/7988\tPercentage Correct: 76.43\nTraining time: 15593.106372117996s\n"
    }
   ],
   "source": [
    "# TRAIN and TEST SetNet OVER MULTIPLE EPOCHS\n",
    "train_set_size = len(split.train_sampler)\n",
    "val_set_size = len(split.val_sampler)\n",
    "test_set_size = len(split.test_sampler)\n",
    "\n",
    "all_train_losses, all_val_losses, all_test_losses = [],[],[]\n",
    "all_train_correct, all_val_correct, all_test_correct = [],[],[]\n",
    "all_train_results, all_val_results, all_test_results = [],[],[]\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for epoch in range(50):\n",
    "    train_loss, train_correct, train_results = trainvaltest.train_settings_net(\n",
    "        model=setnet,\n",
    "        optimizer=optimizer_setnet, \n",
    "        train_loader=train_loader, \n",
    "        train_sampler=split.train_sampler, \n",
    "        epoch=epoch,\n",
    "        loss_function=loss_func_setnet, \n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    val_loss, val_correct, val_results = trainvaltest.val_settings_net(\n",
    "        model=setnet, \n",
    "        val_loader=val_loader, \n",
    "        val_sampler=split.val_sampler, \n",
    "        epoch=epoch,\n",
    "        loss_function=loss_func_setnet,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    test_loss, test_correct, test_results = trainvaltest.test_settings_net(\n",
    "        model=setnet, \n",
    "        test_loader=test_loader, \n",
    "        test_sampler=split.test_sampler, \n",
    "        epoch=epoch,\n",
    "        loss_function=loss_func_setnet,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    all_train_losses.append(train_loss)\n",
    "    all_val_losses.append(val_loss)\n",
    "    all_test_losses.append(test_loss)\n",
    "    \n",
    "    all_train_correct.append(train_correct)\n",
    "    all_val_correct.append(val_correct)\n",
    "    all_test_correct.append(test_correct)\n",
    "    \n",
    "    all_train_results.append(train_results)\n",
    "    all_val_results.append(val_results)\n",
    "    all_test_results.append(test_results)\n",
    "\n",
    "stop = time.time()\n",
    "print(f\"Training time: {stop - start}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE\n",
    "models_folder = 'saved_models'\n",
    "model_name = '20200903_setnet'\n",
    "results_folder = 'saved_results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE MODEL\n",
    "torch.save(setnet, '%s/%s' % (models_folder, model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE RESULTS\n",
    "all_train_losses_npy = np.array(all_train_losses)\n",
    "all_train_correct_npy = np.array(all_train_correct)\n",
    "all_train_results_npy = np.array(all_train_results)\n",
    "\n",
    "all_val_losses_npy = np.array(all_val_losses)\n",
    "all_val_correct_npy = np.array(all_val_correct)\n",
    "all_val_results_npy = np.array(all_val_results)\n",
    "\n",
    "all_test_losses_npy = np.array(all_test_losses)\n",
    "all_test_correct_npy = np.array(all_test_correct)\n",
    "all_test_results_npy = np.array(all_test_results)\n",
    "\n",
    "fx_labels_npy = np.array(list(dataset.fx_to_label.keys()))\n",
    "\n",
    "np.save(file=('%s/%s/%s' % (results_folder, model_name, 'all_train_losses')), arr=all_train_losses_npy)\n",
    "np.save(file=('%s/%s/%s' % (results_folder, model_name, 'all_train_correct')), arr=all_train_correct_npy)\n",
    "np.save(file=('%s/%s/%s' % (results_folder, model_name, 'all_train_results')), arr=all_train_results_npy)\n",
    "\n",
    "np.save(file=('%s/%s/%s' % (results_folder, model_name, 'all_val_losses')), arr=all_val_losses_npy)\n",
    "np.save(file=('%s/%s/%s' % (results_folder, model_name, 'all_val_correct')), arr=all_val_correct_npy)\n",
    "np.save(file=('%s/%s/%s' % (results_folder, model_name, 'all_val_results')), arr=all_val_results_npy)\n",
    "\n",
    "np.save(file=('%s/%s/%s' % (results_folder, model_name, 'all_test_losses')), arr=all_test_losses_npy)\n",
    "np.save(file=('%s/%s/%s' % (results_folder, model_name, 'all_test_correct')), arr=all_test_correct_npy)\n",
    "np.save(file=('%s/%s/%s' % (results_folder, model_name, 'all_test_results')), arr=all_test_results_npy)\n",
    "\n",
    "np.save(file=('%s/%s/%s' % (results_folder, model_name, 'fx_labels')), arr=fx_labels_npy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('venv': venv)",
   "language": "python",
   "name": "python_defaultSpec_1598522299901"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}